{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e32dd9-e5a8-485d-ab4b-20b0a2e56b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50d36a8-9ebf-4e4b-b54b-da67b414212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    LABEL_MAPPING,\n",
    "    ids2labels,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    save_best_model,\n",
    "    load_best_model,\n",
    "    save_model_remotely\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea68431-6093-42b2-8251-1b5e96ca8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = {\n",
    "    \"sentiment\": ['negative', 'neutral', 'positive'],\n",
    "\t\"question\": ['not_question', 'question'],\n",
    "\t\"curse\": ['curse', 'non-curse'],\n",
    "\t\"emotion\": ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise'],\n",
    "\t\"gibberish\": ['clean', 'mild gibberish', 'word salad'],\n",
    "\t\"offensiveness\": ['non-offensive', 'offensive'],\n",
    "\t\"political_bias\": ['CENTER', 'LEFT', 'RIGHT']\n",
    "}\n",
    "\n",
    "label_to_index = {\n",
    "    \"sentiment\": {label: idx for idx, label in enumerate(one_hot_labels[\"sentiment\"])},\n",
    "\t\"question\": {label: idx for idx, label in enumerate(one_hot_labels[\"question\"])},\n",
    "\t\"curse\": {label: idx for idx, label in enumerate(one_hot_labels[\"curse\"])},\n",
    "\t\"emotion\": {label: idx for idx, label in enumerate(one_hot_labels[\"emotion\"])},\n",
    "\t\"gibberish\": {label: idx for idx, label in enumerate(one_hot_labels[\"gibberish\"])},\n",
    "\t\"offensiveness\": {label: idx for idx, label in enumerate(one_hot_labels[\"offensiveness\"])},\n",
    "\t\"political_bias\": {label: idx for idx, label in enumerate(one_hot_labels[\"political_bias\"])}\n",
    "}\n",
    "\n",
    "one_hot_metadata_size = sum([len(x) for x in one_hot_labels.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f3da8b-e341-4693-9338-47d85801a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiarPlusSingleRobertaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filepath: str,\n",
    "        tokenizer,\n",
    "        str_metadata_cols: list[str],\n",
    "        num_metadata_cols: list[str],\n",
    "        one_hot_metadata_cols: list[str],\n",
    "        max_length: int = 512,\n",
    "    ):\n",
    "        self.df = pd.read_csv(filepath)\n",
    "\n",
    "        self.str_metadata_cols = str_metadata_cols\n",
    "        self.num_metadata_cols = num_metadata_cols\n",
    "        self.one_hot_metadata_cols = one_hot_metadata_cols\n",
    "\n",
    "        for column in self.str_metadata_cols:\n",
    "            self.df[column] = self.df[column].astype(str)\n",
    "\n",
    "        self.df[\"statement\"] = self.df[\"statement\"].astype(str)\n",
    "        self.df[\"justification\"] = self.df[\"justification\"].astype(str)\n",
    "        #self.df[\"articles\"] = self.df[\"articles\"].astype(str)\n",
    "\n",
    "        self.statement_max_len = max_length // 4\n",
    "        self.justification_max_len = max_length // 4\n",
    "        #self.article_max_len = max_length // 4\n",
    "        self.str_metadata_max_len = max((\n",
    "            max_length - self.statement_max_len - self.justification_max_len# - self.article_max_len\n",
    "        ) // len(str_metadata_cols), 15)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "        \n",
    "    def limit_tokens(self, text, max_length=512):\n",
    "        return self.tokenizer.convert_tokens_to_string(\n",
    "            self.tokenizer.tokenize(text)[:max_length]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.df.iloc[index]\n",
    "\n",
    "        input_text = self.limit_tokens(\n",
    "            f\"[STATEMENT] {item['statement']}\",\n",
    "            self.statement_max_len\n",
    "        )\n",
    "        input_text += self.limit_tokens(\n",
    "            f\" [JUSTIFICATION] {item['justification']}\",\n",
    "            self.justification_max_len,\n",
    "        )\n",
    "        #input_text += self.limit_tokens(\n",
    "        #    f\" [ARTICLE] {item['articles']}\",\n",
    "        #    self.article_max_len,\n",
    "        #)\n",
    "\n",
    "        for column in self.str_metadata_cols:\n",
    "            input_text += self.limit_tokens(f\" [{column.upper()}] {item[column]}\", self.str_metadata_max_len)\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            input_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        label = LABEL_MAPPING[item[\"label\"]]\n",
    "\n",
    "        num_metadata = [item[column] for column in self.num_metadata_cols]\n",
    "\n",
    "        one_hot_metadata = []\n",
    "        for column in self.one_hot_metadata_cols:\n",
    "            value = item[column]\n",
    "            possible_values = len(one_hot_labels[column])\n",
    "            id_tensor = torch.tensor(label_to_index[column][value])\n",
    "            one_hot_metadata.append(F.one_hot(id_tensor, possible_values))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"num_metadata\": torch.tensor(num_metadata).float(),\n",
    "            \"one_hot_metadata\": torch.cat(one_hot_metadata, dim=0).float(),\n",
    "            \"label\": torch.tensor(label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac266f37-607b-4e14-91fd-6ddba2eeb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiarPlusSingleFinetunedRoBERTasClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, encoder_model, num_metadata_len, one_hot_metadata_size, num_hidden, num_classes\n",
    "    ):\n",
    "        super(LiarPlusSingleFinetunedRoBERTasClassifier, self).__init__()\n",
    "        self.encoder = encoder_model\n",
    "        self.hl = nn.Linear(\n",
    "            self.encoder.config.hidden_size + num_metadata_len + one_hot_metadata_size, num_hidden\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.fc = nn.Linear(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, num_metadata, one_hot_metadata):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        cls_embedding = outputs.pooler_output\n",
    "        concatted_inputs = torch.cat([cls_embedding, num_metadata, one_hot_metadata], dim=1)\n",
    "\n",
    "        hl_output = F.gelu(self.hl(concatted_inputs))\n",
    "        hl_output = self.dropout(hl_output)\n",
    "\n",
    "        logits = self.fc(hl_output)\n",
    "        return logits\n",
    "\n",
    "    def roberta_trainable_state(self):\n",
    "        return {\n",
    "            name: param for name, param in self.encoder.named_parameters() if param.requires_grad\n",
    "        }\n",
    "    \n",
    "    def load_roberta_trainable_state(self, state_dict):\n",
    "        self.encoder.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    # Zapisz tylko wagi warstw klasyfikatora\n",
    "    def state_for_save(self):\n",
    "        return {\n",
    "            'hl_state_dict': self.hl.state_dict(),\n",
    "            'fc_state_dict': self.fc.state_dict(),\n",
    "            'roberta_trainable': self.roberta_trainable_state(),\n",
    "        }\n",
    "        \n",
    "    # Ładowanie modelu (tylko wagi klasyfikatora)\n",
    "    def load_state_from_save(self, state):\n",
    "        self.hl.load_state_dict(state['hl_state_dict'])\n",
    "        self.fc.load_state_dict(state['fc_state_dict'])\n",
    "        if 'roberta_trainable' in state:\n",
    "            self.load_roberta_trainable_state(state['roberta_trainable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1918be92-86fa-40ec-ac25-f941a97140f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: nn.Module,\n",
    "    best_model_path: str,\n",
    "    dataloader: DataLoader,\n",
    "    name: str=\"Test\"\n",
    ") -> None:\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    load_best_model(model, best_model_path)\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    f1 = MulticlassF1Score(num_classes, average=None).to(device)\n",
    "    precision = MulticlassPrecision(num_classes, average=None).to(device)\n",
    "    recall = MulticlassRecall(num_classes, average=None).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            num_metadata = batch[\"num_metadata\"].to(device)\n",
    "            one_hot_metadata = batch[\"one_hot_metadata\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, num_metadata, one_hot_metadata)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += input_ids.size(0)\n",
    "\n",
    "            f1.update(preds, labels)\n",
    "            precision.update(preds, labels)\n",
    "            recall.update(preds, labels)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    f1_res = f1.compute()\n",
    "    precision_res = precision.compute()\n",
    "    recall_res = recall.compute()\n",
    "    \n",
    "    macro_f1 = f1_res.mean()\n",
    "    macro_precision = precision_res.mean()\n",
    "    macro_recall = recall_res.mean()\n",
    "\n",
    "    print(\n",
    "        f\"{name} Accuracy: {accuracy:.4f},\\n\"\n",
    "        f\"{name} Loss: {avg_loss:.4f},\\n\"\n",
    "        f\"{name} F1: {f1_res} (marcro = {macro_f1:.4f}),\\n\"\n",
    "        f\"{name} Precision: {precision_res} (marcro = {macro_precision:.4f}),\\n\"\n",
    "        f\"{name} Recall: {recall_res} (marcro = {macro_recall:.4f}),\\n\"\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        accuracy,\n",
    "        avg_loss,\n",
    "        macro_f1,\n",
    "        macro_precision,\n",
    "        macro_recall\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "992dc4de-c378-485e-8f9f-a61de08f6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_classes = 6\n",
    "hidden_size = 128\n",
    "batch_size = 64\n",
    "\n",
    "text_columns = [\n",
    "    \"subject\",\n",
    "    \"speaker\",\n",
    "    \"job_title\",\n",
    "    \"state\",\n",
    "    \"party_affiliation\",\n",
    "    \"context\"\n",
    "]\n",
    "num_metadata_cols = [\n",
    "    \"barely_true_counts\",\n",
    "    \"false_counts\",\n",
    "    \"half_true_counts\",\n",
    "    \"mostly_true_counts\",\n",
    "    \"pants_on_fire_counts\",\n",
    "    \"grammar_errors\",\n",
    "    \"ratio_of_capital_letters\"\n",
    "]\n",
    "one_hot_cols = [\n",
    "    \"sentiment\",\n",
    "    \"question\",\n",
    "    \"curse\",\n",
    "    \"emotion\",\n",
    "    \"gibberish\",\n",
    "    \"offensiveness\",\n",
    "    \"political_bias\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "242a370d-86a9-4074-bbdc-c503bbf6d373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# trenuje 2 ostatnie warstwy\n",
    "for name, param in roberta.named_parameters():\n",
    "    if name.startswith(\"encoder.layer.11\") or name.startswith(\"pooler\"):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fd8457c-4414-494c-983d-76ad5c0ea9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = LiarPlusSingleRobertaDataset(\n",
    "    \"data/normalized/val2.csv\",\n",
    "    tokenizer,\n",
    "    text_columns,\n",
    "    num_metadata_cols,\n",
    "    one_hot_cols\n",
    ")\n",
    "test_data = LiarPlusSingleRobertaDataset(\n",
    "    \"data/normalized/test2.csv\",\n",
    "    tokenizer,\n",
    "    text_columns,\n",
    "    num_metadata_cols,\n",
    "    one_hot_cols\n",
    ")\n",
    "\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    validation_data, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f032f827-6c35-4dad-91bf-8a3e8c9fe577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiarPlusSingleFinetunedRoBERTasClassifier(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (hl): Linear(in_features=797, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LiarPlusSingleFinetunedRoBERTasClassifier(\n",
    "    roberta,\n",
    "    len(num_metadata_cols),\n",
    "    one_hot_metadata_size,\n",
    "    hidden_size,\n",
    "    num_classes,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51f8920e-c9f5-4de1-ace1-5bb12c37a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Studia\\nauka\\Sztuczna Inteligencja\\praca inżynierska\\klasyfikator\\utils.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"results/FinalSMJ/best_model_6.pth\"\n",
    "load_best_model(model, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6ec174e-ca8c-4cbe-b4ac-86ce6501ac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                                               | 0/21 [00:00<?, ?it/s]E:\\anaconda3\\envs\\ML\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 21/21 [00:38<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.3055,\n",
      "Test Loss: 1.6325,\n",
      "Test F1: tensor([0.2963, 0.3379, 0.2165, 0.3270, 0.3187, 0.2893], device='cuda:0') (marcro = 0.2976),\n",
      "Test Precision: tensor([0.4651, 0.2743, 0.2774, 0.3320, 0.3162, 0.3053], device='cuda:0') (marcro = 0.3284),\n",
      "Test Recall: tensor([0.2174, 0.4400, 0.1776, 0.3221, 0.3213, 0.2749], device='cuda:0') (marcro = 0.2922),\n",
      "\n",
      "0.3055339049103663\n",
      "1.6324805151366295\n",
      "0.2976267337799072\n",
      "0.3283863067626953\n",
      "0.29220423102378845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = test(model, best_model_path, test_dataloader)\n",
    "print('\\n'.join([str(float(x)) for x in res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c544df70-7f42-44f5-9bbf-18f7bd408b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 21/21 [00:38<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3294,\n",
      "Validation Loss: 1.6097,\n",
      "Validation F1: tensor([0.3580, 0.3724, 0.2228, 0.3301, 0.3748, 0.2741], device='cuda:0') (marcro = 0.3220),\n",
      "Validation Precision: tensor([0.6304, 0.3077, 0.3279, 0.3184, 0.3603, 0.2701], device='cuda:0') (marcro = 0.3691),\n",
      "Validation Recall: tensor([0.2500, 0.4715, 0.1688, 0.3427, 0.3904, 0.2781], device='cuda:0') (marcro = 0.3169),\n",
      "\n",
      "0.3294392523364486\n",
      "1.6096927822564622\n",
      "0.3220248222351074\n",
      "0.36912617087364197\n",
      "0.3169243335723877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = test(model, best_model_path, val_dataloader, \"Validation\")\n",
    "print('\\n'.join([str(float(x)) for x in res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fe98c26-0f23-4858-a4c7-efc48c707ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "input_ids = batch[\"input_ids\"].to(device)\n",
    "attention_mask = batch[\"attention_mask\"].to(device)\n",
    "num_metadata = batch[\"num_metadata\"].to(device)\n",
    "one_hot_metadata = batch[\"one_hot_metadata\"].to(device)\n",
    "labels = batch[\"label\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6957f69b-4f3e-4802-b0a6-5ac64909d843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SMJ_Graph.png'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input_ids, attention_mask, num_metadata, one_hot_metadata)  # lub konkretnie np. loss\n",
    "make_dot(output, params=dict(model.named_parameters())).render(\"SMJ_Graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8534cee-a807-4de3-80ed-d4ffb99ad208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best model checkpoint.\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"results/FinalSMJ/best_model_10.pth\"\n",
    "load_best_model(model, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afceccd4-7cb0-47b1-adaf-2363d8a2e12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 21/21 [02:37<00:00,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.2899,\n",
      "Test Loss: 1.6216,\n",
      "Test F1: tensor([0.3262, 0.3592, 0.2376, 0.2760, 0.2747, 0.2679], device='cuda:0') (marcro = 0.2903),\n",
      "Test Precision: tensor([0.4694, 0.3161, 0.2526, 0.2786, 0.2764, 0.2705], device='cuda:0') (marcro = 0.3106),\n",
      "Test Recall: tensor([0.2500, 0.4160, 0.2243, 0.2734, 0.2731, 0.2654], device='cuda:0') (marcro = 0.2837),\n",
      "\n",
      "0.2899454403741232\n",
      "1.6216105546044448\n",
      "0.2902979254722595\n",
      "0.31061816215515137\n",
      "0.28370043635368347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = test(model, best_model_path, test_dataloader)\n",
    "print('\\n'.join([str(float(x)) for x in res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7cb0a07-b815-4180-8024-de7ff176f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 21/21 [02:37<00:00,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3271,\n",
      "Validation Loss: 1.5962,\n",
      "Validation F1: tensor([0.4211, 0.3596, 0.2893, 0.3195, 0.3368, 0.2659], device='cuda:0') (marcro = 0.3320),\n",
      "Validation Precision: tensor([0.6545, 0.3111, 0.3631, 0.2993, 0.3475, 0.2500], device='cuda:0') (marcro = 0.3709),\n",
      "Validation Recall: tensor([0.3103, 0.4259, 0.2405, 0.3427, 0.3267, 0.2840], device='cuda:0') (marcro = 0.3217),\n",
      "\n",
      "0.32710280373831774\n",
      "1.5962113182864086\n",
      "0.3320292830467224\n",
      "0.37091121077537537\n",
      "0.3216942548751831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = test(model, best_model_path, val_dataloader, \"Validation\")\n",
    "print('\\n'.join([str(float(x)) for x in res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122da60f-f93a-4172-ba22-3058bb102ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
