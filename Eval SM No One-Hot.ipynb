{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e32dd9-e5a8-485d-ab4b-20b0a2e56b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50d36a8-9ebf-4e4b-b54b-da67b414212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    LABEL_MAPPING,\n",
    "    ids2labels,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    save_best_model,\n",
    "    load_best_model,\n",
    "    save_model_remotely\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea68431-6093-42b2-8251-1b5e96ca8a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = {\n",
    "    \"sentiment\": ['negative', 'neutral', 'positive'],\n",
    "\t\"question\": ['not_question', 'question'],\n",
    "\t\"curse\": ['curse', 'non-curse'],\n",
    "\t\"emotion\": ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise'],\n",
    "\t\"gibberish\": ['clean', 'mild gibberish', 'word salad'],\n",
    "\t\"offensiveness\": ['non-offensive', 'offensive'],\n",
    "\t\"political_bias\": ['CENTER', 'LEFT', 'RIGHT']\n",
    "}\n",
    "\n",
    "label_to_index = {\n",
    "    \"sentiment\": {label: idx for idx, label in enumerate(one_hot_labels[\"sentiment\"])},\n",
    "\t\"question\": {label: idx for idx, label in enumerate(one_hot_labels[\"question\"])},\n",
    "\t\"curse\": {label: idx for idx, label in enumerate(one_hot_labels[\"curse\"])},\n",
    "\t\"emotion\": {label: idx for idx, label in enumerate(one_hot_labels[\"emotion\"])},\n",
    "\t\"gibberish\": {label: idx for idx, label in enumerate(one_hot_labels[\"gibberish\"])},\n",
    "\t\"offensiveness\": {label: idx for idx, label in enumerate(one_hot_labels[\"offensiveness\"])},\n",
    "\t\"political_bias\": {label: idx for idx, label in enumerate(one_hot_labels[\"political_bias\"])}\n",
    "}\n",
    "\n",
    "one_hot_metadata_size = sum([len(x) for x in one_hot_labels.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8f3da8b-e341-4693-9338-47d85801a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiarPlusSingleRobertaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filepath: str,\n",
    "        tokenizer,\n",
    "        str_metadata_cols: list[str],\n",
    "        num_metadata_cols: list[str],\n",
    "#        one_hot_metadata_cols: list[str],\n",
    "        max_length: int = 512,\n",
    "    ):\n",
    "        self.df = pd.read_csv(filepath)\n",
    "\n",
    "        self.str_metadata_cols = str_metadata_cols\n",
    "        self.num_metadata_cols = num_metadata_cols\n",
    "        #self.one_hot_metadata_cols = one_hot_metadata_cols\n",
    "\n",
    "        for column in self.str_metadata_cols:\n",
    "            self.df[column] = self.df[column].astype(str)\n",
    "\n",
    "        self.df[\"statement\"] = self.df[\"statement\"].astype(str)\n",
    "        #self.df[\"justification\"] = self.df[\"justification\"].astype(str)\n",
    "        #self.df[\"articles\"] = self.df[\"articles\"].astype(str)\n",
    "\n",
    "        self.statement_max_len = max_length // 4\n",
    "        #self.justification_max_len = max_length // 4\n",
    "        #self.article_max_len = max_length // 4\n",
    "        self.str_metadata_max_len = max((\n",
    "            max_length - self.statement_max_len# - self.article_max_len# - self.justification_max_len\n",
    "        ) // len(str_metadata_cols), 15)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "        \n",
    "    def limit_tokens(self, text, max_length=512):\n",
    "        return self.tokenizer.convert_tokens_to_string(\n",
    "            self.tokenizer.tokenize(text)[:max_length]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.df.iloc[index]\n",
    "\n",
    "        input_text = self.limit_tokens(\n",
    "            f\"[STATEMENT] {item['statement']}\",\n",
    "            self.statement_max_len\n",
    "        )\n",
    "        #input_text += self.limit_tokens(\n",
    "        #    f\" [JUSTIFICATION] {item['justification']}\",\n",
    "        #    self.justification_max_len,\n",
    "        #)\n",
    "        #input_text += self.limit_tokens(\n",
    "        #    f\" [ARTICLE] {item['articles']}\",\n",
    "        #    self.article_max_len,\n",
    "        #)\n",
    "\n",
    "        for column in self.str_metadata_cols:\n",
    "            input_text += self.limit_tokens(f\" [{column.upper()}] {item[column]}\", self.str_metadata_max_len)\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            input_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        label = LABEL_MAPPING[item[\"label\"]]\n",
    "\n",
    "        num_metadata = [item[column] for column in self.num_metadata_cols]\n",
    "\n",
    "        #one_hot_metadata = []\n",
    "        #for column in self.one_hot_metadata_cols:\n",
    "        #    value = item[column]\n",
    "        #    possible_values = len(one_hot_labels[column])\n",
    "        #    id_tensor = torch.tensor(label_to_index[column][value])\n",
    "        #    one_hot_metadata.append(F.one_hot(id_tensor, possible_values))\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"num_metadata\": torch.tensor(num_metadata).float(),\n",
    "            #\"one_hot_metadata\": torch.cat(one_hot_metadata, dim=0).float(),\n",
    "            \"label\": torch.tensor(label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac266f37-607b-4e14-91fd-6ddba2eeb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiarPlusSingleFinetunedRoBERTasClassifier(nn.Module):\n",
    "    #one_hot_metadata_size, \n",
    "    def __init__(\n",
    "        self, encoder_model, num_metadata_len, num_hidden, num_classes\n",
    "    ):\n",
    "        super(LiarPlusSingleFinetunedRoBERTasClassifier, self).__init__()\n",
    "        self.encoder = encoder_model\n",
    "        # + one_hot_metadata_size\n",
    "        self.hl = nn.Linear(\n",
    "            self.encoder.config.hidden_size + num_metadata_len, num_hidden\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.fc = nn.Linear(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, num_metadata):#, one_hot_metadata):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        cls_embedding = outputs.pooler_output\n",
    "        #, one_hot_metadata\n",
    "        concatted_inputs = torch.cat([cls_embedding, num_metadata], dim=1)\n",
    "\n",
    "        hl_output = F.gelu(self.hl(concatted_inputs))\n",
    "        hl_output = self.dropout(hl_output)\n",
    "\n",
    "        logits = self.fc(hl_output)\n",
    "        return logits\n",
    "\n",
    "    def roberta_trainable_state(self):\n",
    "        return {\n",
    "            name: param for name, param in self.encoder.named_parameters() if param.requires_grad\n",
    "        }\n",
    "    \n",
    "    def load_roberta_trainable_state(self, state_dict):\n",
    "        self.encoder.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    # Zapisz tylko wagi warstw klasyfikatora\n",
    "    def state_for_save(self):\n",
    "        return {\n",
    "            'hl_state_dict': self.hl.state_dict(),\n",
    "            'fc_state_dict': self.fc.state_dict(),\n",
    "            'roberta_trainable': self.roberta_trainable_state(),\n",
    "        }\n",
    "        \n",
    "    # Ładowanie modelu (tylko wagi klasyfikatora)\n",
    "    def load_state_from_save(self, state):\n",
    "        self.hl.load_state_dict(state['hl_state_dict'])\n",
    "        self.fc.load_state_dict(state['fc_state_dict'])\n",
    "        if 'roberta_trainable' in state:\n",
    "            self.load_roberta_trainable_state(state['roberta_trainable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1918be92-86fa-40ec-ac25-f941a97140f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: nn.Module,\n",
    "    best_model_path: str,\n",
    "    dataloader: DataLoader,\n",
    "    name: str='Test'\n",
    ") -> None:\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    load_best_model(model, best_model_path)\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    f1 = MulticlassF1Score(num_classes, average=None).to(device)\n",
    "    precision = MulticlassPrecision(num_classes, average=None).to(device)\n",
    "    recall = MulticlassRecall(num_classes, average=None).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            num_metadata = batch[\"num_metadata\"].to(device)\n",
    "            #one_hot_metadata = batch[\"one_hot_metadata\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, num_metadata)#, one_hot_metadata)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += input_ids.size(0)\n",
    "\n",
    "            f1.update(preds, labels)\n",
    "            precision.update(preds, labels)\n",
    "            recall.update(preds, labels)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    f1_res = f1.compute()\n",
    "    precision_res = precision.compute()\n",
    "    recall_res = recall.compute()\n",
    "\n",
    "    macro_f1 = f1_res.mean()\n",
    "    macro_precision = precision_res.mean()\n",
    "    macro_recall = recall_res.mean()\n",
    "\n",
    "    print(\n",
    "        f\"{name} Accuracy: {accuracy:.4f},\\n\"\n",
    "        f\"{name} Loss: {avg_loss:.4f},\\n\"\n",
    "        f\"{name} F1: {f1_res} (marcro = {macro_f1:.4f}),\\n\"\n",
    "        f\"{name} Precision: {precision_res} (marcro = {macro_precision:.4f}),\\n\"\n",
    "        f\"{name} Recall: {recall_res} (marcro = {macro_recall:.4f}),\\n\"\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        accuracy,\n",
    "        avg_loss,\n",
    "        macro_f1,\n",
    "        macro_precision,\n",
    "        macro_recall\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "992dc4de-c378-485e-8f9f-a61de08f6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_classes = 6\n",
    "hidden_size = 128\n",
    "batch_size = 64\n",
    "\n",
    "text_columns = [\n",
    "    \"subject\",\n",
    "    \"speaker\",\n",
    "    \"job_title\",\n",
    "    \"state\",\n",
    "    \"party_affiliation\",\n",
    "    \"context\",\n",
    "    \"sentiment\",\n",
    "    \"question\",\n",
    "    \"curse\",\n",
    "    \"emotion\",\n",
    "    \"gibberish\",\n",
    "    \"offensiveness\",\n",
    "    \"political_bias\"\n",
    "]\n",
    "num_metadata_cols = [\n",
    "    \"barely_true_counts\",\n",
    "    \"false_counts\",\n",
    "    \"half_true_counts\",\n",
    "    \"mostly_true_counts\",\n",
    "    \"pants_on_fire_counts\",\n",
    "    \"grammar_errors\",\n",
    "    \"ratio_of_capital_letters\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "242a370d-86a9-4074-bbdc-c503bbf6d373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# trenuje 2 ostatnie warstwy\n",
    "for name, param in roberta.named_parameters():\n",
    "    if name.startswith(\"encoder.layer.11\") or name.startswith(\"pooler\"):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fd8457c-4414-494c-983d-76ad5c0ea9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = LiarPlusSingleRobertaDataset(\n",
    "    \"data/normalized/val2.csv\",\n",
    "    tokenizer,\n",
    "    text_columns,\n",
    "    num_metadata_cols\n",
    ")\n",
    "test_data = LiarPlusSingleRobertaDataset(\n",
    "    \"data/normalized/test2.csv\",\n",
    "    tokenizer,\n",
    "    text_columns,\n",
    "    num_metadata_cols\n",
    ")\n",
    "\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    validation_data, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f032f827-6c35-4dad-91bf-8a3e8c9fe577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiarPlusSingleFinetunedRoBERTasClassifier(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (hl): Linear(in_features=775, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LiarPlusSingleFinetunedRoBERTasClassifier(\n",
    "    roberta,\n",
    "    len(num_metadata_cols),\n",
    "    hidden_size,\n",
    "    num_classes,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51f8920e-c9f5-4de1-ace1-5bb12c37a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Studia\\nauka\\Sztuczna Inteligencja\\praca inżynierska\\klasyfikator\\utils.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"results/FinalSM_NoOneHot/best_model_6.pth\"\n",
    "load_best_model(model, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68401844-44d2-47ed-bed9-210a74c6037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                                               | 0/21 [00:00<?, ?it/s]E:\\anaconda3\\envs\\ML\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 21/21 [00:37<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.2938,\n",
      "Test Loss: 1.6196,\n",
      "Test F1: tensor([0.4048, 0.3340, 0.2302, 0.2601, 0.3662, 0.1449], device='cuda:0') (marcro = 0.2900),\n",
      "Test Precision: tensor([0.4474, 0.3282, 0.2365, 0.2545, 0.2968, 0.3077], device='cuda:0') (marcro = 0.3118),\n",
      "Test Recall: tensor([0.3696, 0.3400, 0.2243, 0.2659, 0.4779, 0.0948], device='cuda:0') (marcro = 0.2954),\n",
      "\n",
      "0.2938425565081839\n",
      "1.619554042444504\n",
      "0.2900201082229614\n",
      "0.31182295083999634\n",
      "0.29541337490081787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = test(model, best_model_path, test_dataloader)\n",
    "print('\\n'.join([str(float(x)) for x in res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c544df70-7f42-44f5-9bbf-18f7bd408b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 21/21 [00:37<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3310,\n",
      "Validation Loss: 1.5928,\n",
      "Validation F1: tensor([0.4700, 0.3377, 0.2557, 0.3197, 0.3840, 0.2222], device='cuda:0') (marcro = 0.3316),\n",
      "Validation Precision: tensor([0.5595, 0.3333, 0.2786, 0.2966, 0.3209, 0.4000], device='cuda:0') (marcro = 0.3648),\n",
      "Validation Recall: tensor([0.4052, 0.3422, 0.2363, 0.3468, 0.4781, 0.1538], device='cuda:0') (marcro = 0.3271),\n",
      "\n",
      "0.3309968847352025\n",
      "1.5927545344718148\n",
      "0.3315572738647461\n",
      "0.36481189727783203\n",
      "0.32706212997436523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = test(model, best_model_path, val_dataloader, \"Validation\")\n",
    "print('\\n'.join([str(float(x)) for x in res]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "281d0aff-11c7-4d47-b131-75b071ddbe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "input_ids = batch[\"input_ids\"].to(device)\n",
    "attention_mask = batch[\"attention_mask\"].to(device)\n",
    "num_metadata = batch[\"num_metadata\"].to(device)\n",
    "labels = batch[\"label\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5625740-f9f0-4d7c-9fc3-af000cacc1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SM_NoOneHot_Graph.png'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(input_ids, attention_mask, num_metadata)  # lub konkretnie np. loss\n",
    "make_dot(output, params=dict(model.named_parameters())).render(\"SM_NoOneHot_Graph\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30285069-f730-44cb-9335-ba60715417ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LiarPlusSingleFinetunedRoBERTasClassifier(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (hl): Linear(in_features=775, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load RoBERTa tokenizer and model\n",
    "tokenizer2 = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta2 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# trenuje 2 ostatnie warstwy\n",
    "for name, param in roberta2.named_parameters():\n",
    "    if name.startswith(\"encoder.layer.11\") or name.startswith(\"pooler\"):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model2 = LiarPlusSingleFinetunedRoBERTasClassifier(\n",
    "    roberta2,\n",
    "    len(num_metadata_cols),\n",
    "    hidden_size,\n",
    "    num_classes,\n",
    ")\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bc4ca3d-d66e-464c-add6-853f96923b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from best model checkpoint.\n"
     ]
    }
   ],
   "source": [
    "load_best_model(model2, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3709c4b-7af8-443d-821d-4cd49bbf8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_models(model1, model2, atol=1e-8):\n",
    "    params1 = list(model1.parameters())\n",
    "    params2 = list(model2.parameters())\n",
    "    \n",
    "    if len(params1) != len(params2):\n",
    "        return False\n",
    "\n",
    "    for p1, p2 in zip(params1, params2):\n",
    "        if not torch.allclose(p1, p2, atol=atol):\n",
    "            return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7408d236-9338-4100-ae21-e3ca39fa73f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_models(model, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44caac9a-af9d-41fa-95a3-a26b746bea54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.720056057117006e-08"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.6195540416082737 - 1.6195540888088342"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef1b9ebb-8ccb-461e-be8e-a8e8cc5cc1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.636433015292596e-08"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.619554042444504 - 1.6195540888088342"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfee17b-10a2-4795-9854-53cb4874820a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
