{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12083438,"sourceType":"datasetVersion","datasetId":7606672}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q mlflow paramiko","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T12:08:32.958400Z","iopub.execute_input":"2025-07-30T12:08:32.958589Z","iopub.status.idle":"2025-07-30T12:08:44.449216Z","shell.execute_reply.started":"2025-07-30T12:08:32.958566Z","shell.execute_reply":"2025-07-30T12:08:44.448275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import RobertaModel, RobertaTokenizer\nfrom sklearn.utils import resample\nfrom torchmetrics.classification import (\n    MulticlassF1Score,\n    MulticlassPrecision,\n    MulticlassRecall,\n)\nfrom tqdm import tqdm\nimport mlflow\nimport time\nimport pandas as pd\nimport os\nimport paramiko","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T12:08:44.451335Z","iopub.execute_input":"2025-07-30T12:08:44.451553Z","iopub.status.idle":"2025-07-30T12:09:10.712246Z","shell.execute_reply.started":"2025-07-30T12:08:44.451535Z","shell.execute_reply":"2025-07-30T12:09:10.711543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LABEL_MAPPING = {\n    \"pants-fire\": 0,\n    \"false\": 1,\n    \"barely-true\": 2,\n    \"half-true\": 3,\n    \"mostly-true\": 4,\n    \"true\": 5,\n}\n\nids2labels = [\n    \"pants-fire\",\n    \"false\",\n    \"barely-true\",\n    \"half-true\",\n    \"mostly-true\",\n    \"true\",\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T12:09:10.713240Z","iopub.execute_input":"2025-07-30T12:09:10.713831Z","iopub.status.idle":"2025-07-30T12:09:10.718191Z","shell.execute_reply.started":"2025-07-30T12:09:10.713792Z","shell.execute_reply":"2025-07-30T12:09:10.717540Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_checkpoint(model, optimizer, epoch, val_acc, path=\"checkpoint.pth\"):\n    checkpoint = {\n        \"model_state_dict\": model.state_for_save(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"epoch\": epoch,\n        \"val_acc\": val_acc,\n    }\n    torch.save(checkpoint, path)\n    print(\n        f\"Checkpoint saved at epoch {epoch} \"\n        f\"with validation accuracy {val_acc:.4f}\"\n    )\n\n\ndef load_checkpoint(\n    model,\n    optimizer,\n    path=\"checkpoint.pth\",\n    resume=False,\n    reset_epoch=False\n):\n    if not resume:\n        print(\"Resume is False. Starting from scratch.\")\n        return 0, 0  # Start fresh\n\n    if os.path.exists(path):\n        checkpoint = torch.load(path)\n        model.load_state_from_save(checkpoint[\"model_state_dict\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        epoch = checkpoint[\"epoch\"]\n        val_acc = checkpoint[\"val_acc\"]\n        if reset_epoch:\n            print(\n                f\"Checkpoint loaded: Starting from initial\"\n                f\"epoch, validation accuracy {val_acc:.4f}\"\n            )\n            return 0, val_acc  # Start fresh with existing model\n        else:\n            print(\n                f\"Checkpoint loaded: Resuming from epoch \"\n                f\"{epoch+1}, validation accuracy {val_acc:.4f}\"\n            )\n            return epoch + 1, val_acc  # Next epoch to train\n    else:\n        print(\"No checkpoint found. Starting from scratch.\")\n        return 0, 0  # Start fresh\n\n\ndef save_best_model(model, optimizer, epoch, val_acc, path=\"best_model.pth\"):\n    best_model = {\n        \"model_state_dict\": model.state_for_save(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"epoch\": epoch,\n        \"val_acc\": val_acc,\n    }\n    torch.save(best_model, path)\n    print(\n        f\"Best model saved at epoch {epoch} \"\n        f\"with validation accuracy {val_acc:.4f}\"\n    )\n\n\ndef load_best_model(model, path=\"best_model.pth\"):\n    if os.path.exists(path):\n        best_model = torch.load(path)\n        model.load_state_from_save(best_model[\"model_state_dict\"])\n        print(\"Model loaded from best model checkpoint.\")\n    else:\n        print(\"No best model checkpoint found.\")\n\n\ndef save_model_remotely(local_path, remote_path, creds):\n    # Ustawienia SSH\n    hostname = creds['hostname']#\"cimmerian.win\"\n    port = creds['port']#22\n    username = creds['username']#\"conan\"\n    password = creds['password']#\"conan\"\n\n    # Połączenie SSH\n    try:\n        ssh = paramiko.SSHClient()\n        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        ssh.connect(hostname, port=port, username=username, password=password)\n    \n        # Pobierz rozmiar pliku lokalnego\n        file_size = os.path.getsize(local_path)\n\n        # Funkcja do aktualizacji paska postępu\n        def progress_callback(transferred, total):\n            progress_bar.update(transferred - progress_bar.n)\n        \n        # Inicjalizuj pasek postępu\n        progress_bar = tqdm(total=file_size, unit='B', unit_scale=True, desc=f\"Uploading {local_path}\")\n    \n        # SFTP transfer z callbackiem\n        with ssh.open_sftp() as sftp:\n            temp_remote_path = remote_path + os.path.basename(local_path) + \".tmp\"\n            final_remote_path = remote_path + os.path.basename(local_path)\n\n            sftp.put(local_path, temp_remote_path, callback=progress_callback)\n\n            try:\n                sftp.remove(final_remote_path)\n            except IOError:\n                # Plik nie istnieje – można ignorować\n                pass\n            \n            sftp.rename(temp_remote_path, final_remote_path)\n\n    \n        # Po zakończeniu\n        progress_bar.close()\n        print(f\"Plik {os.path.basename(local_path)} został wysłany.\")\n    \n    except Exception as e:\n        print(f\"Error: {e}\")\n    \n    finally:\n        # Zapewnia, że połączenie SSH zawsze zostanie zamknięte\n        ssh.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T12:09:10.719151Z","iopub.execute_input":"2025-07-30T12:09:10.719323Z","iopub.status.idle":"2025-07-30T12:09:10.785964Z","shell.execute_reply.started":"2025-07-30T12:09:10.719309Z","shell.execute_reply":"2025-07-30T12:09:10.785146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LiarPlusDataset(Dataset):\n    def __init__(\n        self,\n        filepath: str,\n        tokenizer,\n        columns: list[str],\n        num_metadata_cols: list[str],\n        max_length: int = 128,\n    ):\n        self.df = pd.read_csv(filepath)\n\n        self.columns = columns\n        self.num_metadata_cols = num_metadata_cols\n\n        for column in self.columns:\n            self.df[column] = self.df[column].astype(str)\n\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df.index)\n\n    def __getitem__(self, index: int):\n        item = self.df.iloc[index]\n\n        input_ids = []\n        attention_mask = []\n\n        for column in self.columns:\n            encoded = self.tokenizer(\n                item[column],\n                truncation=True,\n                padding=\"max_length\",\n                max_length=self.max_length,\n                return_tensors=\"pt\",\n            )\n            input_ids.append(encoded[\"input_ids\"])\n            attention_mask.append(encoded[\"attention_mask\"])\n\n        input_ids = torch.cat(input_ids, dim=0)\n        attention_mask = torch.cat(attention_mask, dim=0)\n\n        label = LABEL_MAPPING[item[\"label\"]]\n\n        metadata = [item[column] for column in self.num_metadata_cols]\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"num_metadata\": torch.tensor(metadata).float(),\n            \"label\": torch.tensor(label),\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T12:09:10.786946Z","iopub.execute_input":"2025-07-30T12:09:10.787286Z","iopub.status.idle":"2025-07-30T12:09:10.806388Z","shell.execute_reply.started":"2025-07-30T12:09:10.787262Z","shell.execute_reply":"2025-07-30T12:09:10.805409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LiarPlusDatasetSubset(Dataset):\n    def __init__(\n        self,\n        total_size: int,\n        filepath: str,\n        tokenizer,\n        columns: list[str],\n        num_metadata_cols: list[str],\n        random_state: int | None = None,\n        max_length: int = 128,\n    ):\n        num_classes = 6\n        df = pd.read_csv(filepath)\n\n        if total_size != -1:\n            desired_count = total_size // num_classes\n\n            self.df = pd.concat(\n                [\n                    resample(\n                        group,\n                        replace=False,\n                        n_samples=desired_count,\n                        random_state=random_state,\n                    )\n                    for _, group in df.groupby(\"label\")\n                ]\n            )\n        else:\n            self.df = df\n\n        self.columns = columns\n        self.num_metadata_cols = num_metadata_cols\n\n        for column in self.columns:\n            self.df[column] = self.df[column].astype(str)\n\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.df.index)\n\n    def __getitem__(self, index: int):\n        item = self.df.iloc[index]\n\n        input_ids = []\n        attention_mask = []\n\n        for column in self.columns:\n            encoded = self.tokenizer(\n                item[column],\n                truncation=True,\n                padding=\"max_length\",\n                max_length=self.max_length,\n                return_tensors=\"pt\",\n            )\n            input_ids.append(encoded[\"input_ids\"])\n            attention_mask.append(encoded[\"attention_mask\"])\n\n        input_ids = torch.cat(input_ids, dim=0)\n        attention_mask = torch.cat(attention_mask, dim=0)\n\n        label = LABEL_MAPPING[item[\"label\"]]\n\n        metadata = [item[column] for column in self.num_metadata_cols]\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"num_metadata\": torch.tensor(metadata).float(),\n            \"label\": torch.tensor(label),\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T12:09:10.807358Z","iopub.execute_input":"2025-07-30T12:09:10.807638Z","iopub.status.idle":"2025-07-30T12:09:10.827271Z","shell.execute_reply.started":"2025-07-30T12:09:10.807614Z","shell.execute_reply":"2025-07-30T12:09:10.826328Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LiarPlusMultipleRoBERTasClassifier(nn.Module):\n    def __init__(\n        self, encoder_model, inputs, num_metadata_len, num_hidden, num_classes\n    ):\n        super(LiarPlusMultipleRoBERTasClassifier, self).__init__()\n        self.encoder = encoder_model\n        self.hl = nn.Linear(\n            self.encoder.config.hidden_size * inputs + num_metadata_len,\n            num_hidden,\n        )\n        self.fc = nn.Linear(num_hidden, num_classes)\n\n    def forward(self, input_ids, attention_mask, num_metadata):\n        batch_size, num_fields, max_length = input_ids.shape\n\n        # reshape from (batch_size, num_fields, max_length) to (batch_size * num_fields, max_length)\n        flat_input_ids = input_ids.view(batch_size * num_fields, max_length)\n        flat_attention_mask = attention_mask.view(\n            batch_size * num_fields, max_length\n        )\n\n        with torch.no_grad():  # Ensure encoder remains frozen\n            outputs = self.encoder(\n                input_ids=flat_input_ids, attention_mask=flat_attention_mask\n            )\n\n        # hidden_size should be 768 for RoBERTa\n        # shape (batch_size * num_fields, hidden_size)\n        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n\n        # reshape (batch_size * num_fields, hidden_size) -> (batch_size, num_fields, hidden_size)\n        cls_reshaped = cls_embeddings.view(batch_size, num_fields, -1)\n\n        # reshape (batch_size, num_fields, hidden_size) -> (batch_size, num_fields * hidden_size)\n        # which is concatenation along seperate fields' CLS token for following classification\n        flattened_cls = torch.flatten(cls_reshaped, start_dim=1)\n\n        concatted_inputs = torch.cat([flattened_cls, num_metadata], dim=1)\n\n        # pass through hidden layer for better feature selection\n        hl_output = F.gelu(self.hl(concatted_inputs))\n\n        # pass through classification layer\n        logits = self.fc(hl_output)\n\n        return logits\n\n    # Zapisz tylko wagi warstw klasyfikatora\n    def state_for_save(self):\n        return {\n            'hl_state_dict': self.hl.state_dict(),\n            'fc_state_dict': self.fc.state_dict(),\n        }\n        \n    # Ładowanie modelu (tylko wagi klasyfikatora)\n    def load_state_from_save(self, state):\n        self.hl.load_state_dict(state['hl_state_dict'])\n        self.fc.load_state_dict(state['fc_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T12:09:10.829246Z","iopub.execute_input":"2025-07-30T12:09:10.829797Z","iopub.status.idle":"2025-07-30T12:09:10.848974Z","shell.execute_reply.started":"2025-07-30T12:09:10.829779Z","shell.execute_reply":"2025-07-30T12:09:10.848354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(\n    model: nn.Module,\n    best_model_path: str,\n    dataloader: DataLoader\n) -> None:\n    # Define loss function\n    criterion = nn.CrossEntropyLoss()\n\n    load_best_model(model, best_model_path)\n    \n    model.eval()  # Set model to evaluation mode\n    total_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n\n    f1 = MulticlassF1Score(num_classes, average=None).to(device)\n    precision = MulticlassPrecision(num_classes, average=None).to(device)\n    recall = MulticlassRecall(num_classes, average=None).to(device)\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            num_metadata = batch[\"num_metadata\"].to(device)\n            labels = batch[\"label\"].to(device)\n\n            outputs = model(input_ids, attention_mask, num_metadata)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * input_ids.size(0)\n\n            preds = torch.argmax(outputs, dim=1)\n            total_correct += (preds == labels).sum().item()\n            total_samples += input_ids.size(0)\n\n            f1.update(preds, labels)\n            precision.update(preds, labels)\n            recall.update(preds, labels)\n\n    avg_loss = total_loss / total_samples\n    accuracy = total_correct / total_samples\n\n    f1_res = f1.compute()\n    precision_res = precision.compute()\n    recall_res = recall.compute()\n\n    mlflow.log_metric(\"test_acc\", accuracy)\n    mlflow.log_metric(\"test_loss\", accuracy)\n\n    for i in range(num_classes):\n        mlflow.log_metric(f\"test_f1_{ids2labels[i]}\", f1_res[i])\n        mlflow.log_metric(f\"test_precision_{ids2labels[i]}\", precision_res[i])\n        mlflow.log_metric(f\"test_recall_{ids2labels[i]}\", recall_res[i])\n    \n    macro_f1 = f1_res.mean()\n    macro_precision = precision_res.mean()\n    macro_recall = recall_res.mean()\n\n    mlflow.log_metric(\"test_f1\", macro_f1)\n    mlflow.log_metric(\"test_precision\", macro_precision)\n    mlflow.log_metric(\"test_recall\", macro_recall)\n\n    print(\n        f\"Test Loss: {avg_loss:.4f}, \"\n        f\"Test Accuracy: {accuracy:.4f}, \"\n        f\"Test F1: {f1_res} (marcro = {macro_f1:.4f}), \"\n        f\"Test Precision: {precision_res} (marcro = {macro_precision:.4f}), \"\n        f\"Test Recall: {recall_res} (marcro = {macro_recall:.4f}), \"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T12:09:10.849736Z","iopub.execute_input":"2025-07-30T12:09:10.850044Z","iopub.status.idle":"2025-07-30T12:09:10.870431Z","shell.execute_reply.started":"2025-07-30T12:09:10.850015Z","shell.execute_reply":"2025-07-30T12:09:10.869549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(\n    creds: dict,\n    model: nn.Module,\n    save_path: str,\n    remote_models_path: str,\n    best_model_path: str,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    test_loader: DataLoader,\n    batch_size: int,\n    num_classes: int,\n    lr=1e-3,\n    epochs=30,\n    patience=5,\n    resume: bool = False,\n    reset_epoch: bool = False,\n) -> None:\n    dev_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device {dev_name}\")\n    device = torch.device(dev_name)\n\n    # Define optimizer and loss function\n    # Train only the classifier\n    optimizer = torch.optim.Adam(model.fc.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    # Checkpoint Path\n    checkpoint_path = f\"{save_path}/checkpoint_{patience}.pth\"\n\n    checkpoint_send_interval = 2\n\n    # Track best loss for model saving\n    # Load Checkpoint (Decide if you want to continue)\n    start_epoch, best_val_accuracy = load_checkpoint(\n        model,\n        optimizer,\n        checkpoint_path,\n        resume,\n        reset_epoch\n    )\n\n    patience_counter = 0\n\n    f1 = MulticlassF1Score(num_classes, average=None).to(device)\n    precision = MulticlassPrecision(num_classes, average=None).to(device)\n    recall = MulticlassRecall(num_classes, average=None).to(device)\n\n    # Training loop\n    for epoch in range(start_epoch, epochs):\n        model.train()\n        epoch_loss = 0\n\n        train_accuracy = 0\n\n        for batch in tqdm(\n            train_loader, desc=f\"Epoch {epoch+1}\", leave=False\n        ):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            num_metadata = batch[\"num_metadata\"].to(device)\n            labels = batch[\"label\"].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask, num_metadata)\n            loss = criterion(\n                outputs, labels\n            )  # można spróbować to logować jako osobny wykres do debugowania\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n            # Calculate accuracy\n            preds = torch.argmax(outputs, dim=-1)\n            train_accuracy += (preds == labels).sum().item()\n\n            f1.update(preds, labels)\n            precision.update(preds, labels)\n            recall.update(preds, labels)\n\n        avg_loss = epoch_loss / len(train_loader)\n        avg_train_accuracy = train_accuracy / len(train_loader.dataset)\n        mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n        mlflow.log_metric(\"train_acc\", avg_train_accuracy, step=epoch)\n\n        f1_res = f1.compute()\n        precision_res = precision.compute()\n        recall_res = recall.compute()\n\n        for i in range(num_classes):\n            mlflow.log_metric(\n                f\"train_f1_{ids2labels[i]}\", f1_res[i], step=epoch\n            )\n            mlflow.log_metric(\n                f\"train_precision_{ids2labels[i]}\",\n                precision_res[i],\n                step=epoch,\n            )\n            mlflow.log_metric(\n                f\"train_recall_{ids2labels[i]}\", recall_res[i], step=epoch\n            )\n\n        macro_f1 = f1_res.mean()\n        macro_precision = precision_res.mean()\n        macro_recall = recall_res.mean()\n\n        mlflow.log_metric(\"train_f1\", macro_f1, step=epoch)\n        mlflow.log_metric(\"train_precision\", macro_precision, step=epoch)\n        mlflow.log_metric(\"train_recall\", macro_recall, step=epoch)\n\n        tqdm.write(\n            f\"Epoch {epoch+1}: \"\n            f\"Training Loss: {avg_loss}, \"\n            f\"Training Accuracy: {avg_train_accuracy}, \"\n            f\"Training F1: {macro_f1}, \"\n            f\"Training Precision: {macro_precision}, \"\n            f\"Training Recall: {macro_recall}\"\n        )\n\n        # Validation step\n        model.eval()  # Switch to evaluation mode\n        val_loss = 0\n        val_accuracy = 0\n\n        f1.reset()\n        precision.reset()\n        recall.reset()\n\n        with torch.no_grad():\n            for batch in tqdm(\n                val_loader,\n                desc=f\"Validation of epoch {epoch + 1}\",\n                leave=False,\n            ):\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n                num_metadata = batch[\"num_metadata\"].to(device)\n                labels = batch[\"label\"].to(device)\n\n                outputs = model(input_ids, attention_mask, num_metadata)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                # Calculate accuracy\n                preds = torch.argmax(outputs, dim=-1)\n                val_accuracy += (preds == labels).sum().item()\n                f1.update(preds, labels)\n                precision.update(preds, labels)\n                recall.update(preds, labels)\n\n        avg_val_loss = val_loss / len(val_loader)\n        avg_val_accuracy = val_accuracy / len(val_loader.dataset)\n        mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch)\n        mlflow.log_metric(\"val_acc\", avg_val_accuracy, step=epoch)\n\n        f1_res = f1.compute()\n        precision_res = precision.compute()\n        recall_res = recall.compute()\n\n        for i in range(num_classes):\n            mlflow.log_metric(\n                f\"val_f1_{ids2labels[i]}\", f1_res[i], step=epoch\n            )\n            mlflow.log_metric(\n                f\"val_precision_{ids2labels[i]}\",\n                precision_res[i],\n                step=epoch,\n            )\n            mlflow.log_metric(\n                f\"val_recall_{ids2labels[i]}\", recall_res[i], step=epoch\n            )\n\n        macro_f1 = f1_res.mean()\n        macro_precision = precision_res.mean()\n        macro_recall = recall_res.mean()\n\n        mlflow.log_metric(\"val_f1\", macro_f1, step=epoch)\n        mlflow.log_metric(\"val_precision\", macro_precision, step=epoch)\n        mlflow.log_metric(\"val_recall\", macro_recall, step=epoch)\n\n        print(\n            f\"Epoch {epoch+1}: \"\n            f\"Validation Loss: {avg_val_loss}, \"\n            f\"Validation Accuracy: {avg_val_accuracy}, \"\n            f\"Validation F1: {macro_f1}, \"\n            f\"Validation Precision: {macro_precision}, \"\n            f\"Validation Recall: {macro_recall}\"\n        )\n\n        save_checkpoint(\n            model, optimizer, epoch, avg_val_accuracy, checkpoint_path\n        )\n        if (epoch + 1) % checkpoint_send_interval == 0:# and epoch != 0:\n            save_model_remotely(checkpoint_path, remote_models_path, creds)\n\n        # Check for early stopping\n        if avg_val_accuracy > best_val_accuracy:\n            best_val_accuracy = avg_val_accuracy\n            patience_counter = 0\n            # Save the best model\n            save_best_model(\n                model,\n                optimizer,\n                epoch,\n                best_val_accuracy,\n                best_model_path\n            )\n            save_model_remotely(best_model_path, remote_models_path, creds)\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    # Log final checkpoint\n    save_model_remotely(checkpoint_path, remote_models_path, creds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T12:09:10.871318Z","iopub.execute_input":"2025-07-30T12:09:10.872174Z","iopub.status.idle":"2025-07-30T12:09:10.895716Z","shell.execute_reply.started":"2025-07-30T12:09:10.872147Z","shell.execute_reply":"2025-07-30T12:09:10.895025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mlflow_uri = \"http://cimmerian.win:5000\"\nresume = False\nreset_epoch = False\n\ncreds = {\n    'hostname': \"cimmerian.win\",\n    'port': 22,\n    'username': \"conan\",\n    'password': \"conan\"\n}\n\nmlflow.set_tracking_uri(uri=mlflow_uri)\n\n# MLflow experiment setup\nmlflow.set_experiment(\"LiarPlusMultipleRoBERTasClassifier\")\n\n# Load RoBERTa tokenizer and model\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nroberta = RobertaModel.from_pretrained(\"roberta-base\")\n\nfor param in roberta.parameters():\n    param.requires_grad = False  # Freeze all layers\n\n# Hyperparameters\nnum_classes = 6\nlr = 1e-3\nepochs = 30\nhidden_size = 128\n# Number of epochs to wait before stopping if no improvement\npatience = 10\n\n# Save path\nsave_path = \"/kaggle/working/\"\n# Remote models path\nremote_models_path = \"/home/conan/models/multiple_robertas/\"\n# Best model path\nbest_model_path = f\"{save_path}/best_model_{patience}.pth\"\n\n# można przetestować zachłannie\n# dodajemy kolumnę jak poprawia i nie dodajemy jak nie poprawia\ntext_columns = [\n    \"statement\",\n    \"subject\",\n    \"speaker\",\n    \"job_title\",\n    \"state\",\n    \"party_affiliation\",\n    \"context\",\n    \"justification\",\n]\nnum_metadata_cols = [\n    \"barely_true_counts\",\n    \"false_counts\",\n    \"half_true_counts\",\n    \"mostly_true_counts\",\n    \"pants_on_fire_counts\",\n]\n\nsubset_size = 1000\nrandom_state = 42\n\n# speedup the experiments\n# można ustawić epochs na 1 i sprawdzić czy w ramach jednej epoki val loss spada\ntraining_data_subset = LiarPlusDatasetSubset(\n    -1,\n    \"/kaggle/input/liar-plus-final-dataset/train2.csv\",\n    tokenizer,\n    text_columns,\n    num_metadata_cols,\n    random_state,\n)\nvalidation_data = LiarPlusDatasetSubset(\n    -1,\n    \"/kaggle/input/liar-plus-final-dataset/val2.csv\",\n    tokenizer,\n    text_columns,\n    num_metadata_cols,\n)\ntest_data = LiarPlusDatasetSubset(\n    -1,\n    \"/kaggle/input/liar-plus-final-dataset/test2.csv\",\n    tokenizer,\n    text_columns,\n    num_metadata_cols,\n)\n\nbatch_size = 64\n\ntrain_dataloader = DataLoader(\n    training_data_subset, batch_size=batch_size, shuffle=True\n)\nval_dataloader = DataLoader(\n    validation_data, batch_size=batch_size, shuffle=True\n)\ntest_dataloader = DataLoader(\n    test_data, batch_size=batch_size, shuffle=True\n)\n\n# Instantiate model\nmodel = LiarPlusMultipleRoBERTasClassifier(\n    roberta,\n    len(text_columns),\n    len(num_metadata_cols),\n    hidden_size,\n    num_classes,\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nstart = time.time()\nwith mlflow.start_run():\n    mlflow.log_param(\"learning_rate\", lr)\n    mlflow.log_param(\"batch_size\", batch_size)\n    mlflow.log_param(\"epochs\", epochs)\n    mlflow.log_param(\"resume\", resume)\n    mlflow.log_param(\"reset_epoch\", reset_epoch)\n    mlflow.log_param(\"patience\", patience)\n    \n    # Train the model\n    train(\n        creds,\n        model,\n        save_path,\n        remote_models_path,\n        best_model_path,\n        train_dataloader,\n        val_dataloader,\n        test_dataloader,\n        batch_size,\n        num_classes,\n        lr,\n        epochs,\n        patience,\n        resume,\n        reset_epoch,\n    )\n    # Evaluate on test dataset\n    test(model, best_model_path, test_dataloader)\nend = time.time()\nprint(f\"Total time took training: {end-start}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T12:19:14.582104Z","iopub.execute_input":"2025-07-30T12:19:14.582388Z","iopub.status.idle":"2025-07-30T12:20:18.302698Z","shell.execute_reply.started":"2025-07-30T12:19:14.582369Z","shell.execute_reply":"2025-07-30T12:20:18.302051Z"}},"outputs":[],"execution_count":null}]}