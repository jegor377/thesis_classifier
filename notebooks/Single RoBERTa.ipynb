{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-12T15:55:28.296911Z",
     "iopub.status.busy": "2025-05-12T15:55:28.296646Z",
     "iopub.status.idle": "2025-05-12T15:55:38.836756Z",
     "shell.execute_reply": "2025-05-12T15:55:38.835809Z",
     "shell.execute_reply.started": "2025-05-12T15:55:28.296866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m700.2/700.2 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q mlflow paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T15:55:38.838784Z",
     "iopub.status.busy": "2025-05-12T15:55:38.838549Z",
     "iopub.status.idle": "2025-05-12T15:56:10.393136Z",
     "shell.execute_reply": "2025-05-12T15:56:10.392583Z",
     "shell.execute_reply.started": "2025-05-12T15:55:38.838764Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 15:55:54.739583: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747065354.947505      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747065355.008082      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from random import sample\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from sklearn.utils import resample\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassF1Score,\n",
    "    MulticlassPrecision,\n",
    "    MulticlassRecall,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:30.235304Z",
     "iopub.status.busy": "2025-05-12T16:05:30.234996Z",
     "iopub.status.idle": "2025-05-12T16:05:30.239493Z",
     "shell.execute_reply": "2025-05-12T16:05:30.238924Z",
     "shell.execute_reply.started": "2025-05-12T16:05:30.235284Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/input/liar-plus-utils')\n",
    "from utils import (\n",
    "    LABEL_MAPPING,\n",
    "    ids2labels,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    save_best_model,\n",
    "    load_best_model,\n",
    "    save_model_remotely\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:30.390328Z",
     "iopub.status.busy": "2025-05-12T16:05:30.390045Z",
     "iopub.status.idle": "2025-05-12T16:05:30.399726Z",
     "shell.execute_reply": "2025-05-12T16:05:30.399092Z",
     "shell.execute_reply.started": "2025-05-12T16:05:30.390307Z"
    }
   },
   "outputs": [],
   "source": [
    "class LiarPlusSingleRobertaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filepath: str,\n",
    "        tokenizer,\n",
    "        str_metadata_cols: list[str],\n",
    "        num_metadata_cols: list[str],\n",
    "        max_length: int = 512,\n",
    "    ):\n",
    "        self.df = pd.read_csv(filepath)\n",
    "\n",
    "        self.str_metadata_cols = str_metadata_cols\n",
    "        self.num_metadata_cols = num_metadata_cols\n",
    "\n",
    "        for column in self.str_metadata_cols:\n",
    "            self.df[column] = self.df[column].astype(str)\n",
    "\n",
    "        self.df[\"statement\"] = self.df[\"statement\"].astype(str)\n",
    "        self.df[\"justification\"] = self.df[\"justification\"].astype(str)\n",
    "\n",
    "        self.statement_max_len = max_length // 4\n",
    "        self.justification_max_len = max_length // 4\n",
    "        self.str_metadata_max_len = (\n",
    "            max_length - self.statement_max_len - self.justification_max_len\n",
    "        ) // len(str_metadata_cols)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def limit_tokens(self, text, max_length=512):\n",
    "        tokenized = self.tokenizer.tokenize(text)[:max_length]\n",
    "        return self.tokenizer.decode(\n",
    "            self.tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.df.iloc[index]\n",
    "\n",
    "        input = self.limit_tokens(\n",
    "            f\"[STATEMENT] {item['statement']} \", self.statement_max_len\n",
    "        )\n",
    "        input += self.limit_tokens(\n",
    "            f\" [JUSTIFICATION] {item['justification']}\",\n",
    "            self.justification_max_len,\n",
    "        )\n",
    "\n",
    "        for column in self.str_metadata_cols:\n",
    "            input += self.limit_tokens(\n",
    "                f\" [{column.upper()}] {item[column]}\",\n",
    "                self.str_metadata_max_len,\n",
    "            )\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            input,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        label = LABEL_MAPPING[item[\"label\"]]\n",
    "\n",
    "        num_metadata = [item[column] for column in self.num_metadata_cols]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"num_metadata\": torch.tensor(num_metadata).float(),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:30.544409Z",
     "iopub.status.busy": "2025-05-12T16:05:30.544132Z",
     "iopub.status.idle": "2025-05-12T16:05:30.551029Z",
     "shell.execute_reply": "2025-05-12T16:05:30.550286Z",
     "shell.execute_reply.started": "2025-05-12T16:05:30.544389Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LiarPlusSingleRoBERTasClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, encoder_model, num_metadata_len, num_hidden, num_classes\n",
    "    ):\n",
    "        super(LiarPlusSingleRoBERTasClassifier, self).__init__()\n",
    "        self.encoder = encoder_model\n",
    "        self.hl = nn.Linear(\n",
    "            self.encoder.config.hidden_size + num_metadata_len, num_hidden\n",
    "        )\n",
    "        self.fc = nn.Linear(num_hidden, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, num_metadata):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(\n",
    "                input_ids=input_ids, attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        concatted_inputs = torch.cat([cls_embedding, num_metadata], dim=1)\n",
    "\n",
    "        hl_output = F.gelu(self.hl(concatted_inputs))\n",
    "\n",
    "        logits = self.fc(hl_output)\n",
    "        return logits\n",
    "\n",
    "    # Zapisz tylko wagi warstw klasyfikatora\n",
    "    def state_for_save(self):\n",
    "        return {\n",
    "            'hl_state_dict': self.hl.state_dict(),\n",
    "            'fc_state_dict': self.fc.state_dict(),\n",
    "        }\n",
    "        \n",
    "    # Åadowanie modelu (tylko wagi klasyfikatora)\n",
    "    def load_state_from_save(self, state):\n",
    "        self.hl.load_state_dict(state['hl_state_dict'])\n",
    "        self.fc.load_state_dict(state['fc_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:30.701654Z",
     "iopub.status.busy": "2025-05-12T16:05:30.700751Z",
     "iopub.status.idle": "2025-05-12T16:05:30.716825Z",
     "shell.execute_reply": "2025-05-12T16:05:30.716123Z",
     "shell.execute_reply.started": "2025-05-12T16:05:30.701615Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: nn.Module,\n",
    "    best_model_path: str,\n",
    "    dataloader: DataLoader\n",
    ") -> None:\n",
    "    # Define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    load_best_model(model, best_model_path)\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    f1 = MulticlassF1Score(num_classes, average=None).to(device)\n",
    "    precision = MulticlassPrecision(num_classes, average=None).to(device)\n",
    "    recall = MulticlassRecall(num_classes, average=None).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            num_metadata = batch[\"num_metadata\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, num_metadata)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += input_ids.size(0)\n",
    "\n",
    "            f1.update(preds, labels)\n",
    "            precision.update(preds, labels)\n",
    "            recall.update(preds, labels)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    f1_res = f1.compute()\n",
    "    precision_res = precision.compute()\n",
    "    recall_res = recall.compute()\n",
    "\n",
    "    mlflow.log_metric(\"test_acc\", accuracy)\n",
    "    mlflow.log_metric(\"test_loss\", accuracy)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        mlflow.log_metric(f\"test_f1_{ids2labels[i]}\", f1_res[i])\n",
    "        mlflow.log_metric(f\"test_precision_{ids2labels[i]}\", precision_res[i])\n",
    "        mlflow.log_metric(f\"test_recall_{ids2labels[i]}\", recall_res[i])\n",
    "    \n",
    "    macro_f1 = f1_res.mean()\n",
    "    macro_precision = precision_res.mean()\n",
    "    macro_recall = recall_res.mean()\n",
    "\n",
    "    mlflow.log_metric(\"test_f1\", macro_f1)\n",
    "    mlflow.log_metric(\"test_precision\", macro_precision)\n",
    "    mlflow.log_metric(\"test_recall\", macro_recall)\n",
    "\n",
    "    print(\n",
    "        f\"Test Loss: {avg_loss:.4f}, \"\n",
    "        f\"Test Accuracy: {accuracy:.4f}, \"\n",
    "        f\"Test F1: {f1_res} (marcro = {macro_f1:.4f}), \"\n",
    "        f\"Test Precision: {precision_res} (marcro = {macro_precision:.4f}), \"\n",
    "        f\"Test Recall: {recall_res} (marcro = {macro_recall:.4f}), \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:05:30.851268Z",
     "iopub.status.busy": "2025-05-12T16:05:30.850771Z",
     "iopub.status.idle": "2025-05-12T16:05:30.875553Z",
     "shell.execute_reply": "2025-05-12T16:05:30.874799Z",
     "shell.execute_reply.started": "2025-05-12T16:05:30.851237Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    creds: dict,\n",
    "    model: nn.Module,\n",
    "    save_path: str,\n",
    "    remote_models_path: str,\n",
    "    best_model_path: str,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    batch_size: int,\n",
    "    num_classes: int,\n",
    "    lr=1e-3,\n",
    "    epochs=30,\n",
    "    patience=5,\n",
    "    resume: bool = False,\n",
    "    reset_epoch: bool = False,\n",
    ") -> None:\n",
    "    dev_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device {dev_name}\")\n",
    "    device = torch.device(dev_name)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    # Train only the classifier\n",
    "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Checkpoint Path\n",
    "    checkpoint_path = f\"checkpoint_{patience}.pth\"\n",
    "\n",
    "    checkpoint_send_interval = 2\n",
    "\n",
    "    # Track best loss for model saving\n",
    "    # Load Checkpoint (Decide if you want to continue)\n",
    "    start_epoch, best_val_accuracy = load_checkpoint(\n",
    "        model,\n",
    "        optimizer,\n",
    "        checkpoint_path,\n",
    "        resume,\n",
    "        reset_epoch\n",
    "    )\n",
    "\n",
    "    patience_counter = 0\n",
    "\n",
    "    f1 = MulticlassF1Score(num_classes, average=None).to(device)\n",
    "    precision = MulticlassPrecision(num_classes, average=None).to(device)\n",
    "    recall = MulticlassRecall(num_classes, average=None).to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        train_accuracy = 0\n",
    "\n",
    "        for batch in tqdm(\n",
    "            train_loader, desc=f\"Epoch {epoch+1}\", leave=False\n",
    "        ):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            num_metadata = batch[\"num_metadata\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, num_metadata)\n",
    "            loss = criterion(\n",
    "                outputs, labels\n",
    "            )  # moÅ¼na sprÃ³bowaÄ‡ to logowaÄ‡ jako osobny wykres do debugowania\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            train_accuracy += (preds == labels).sum().item()\n",
    "\n",
    "            f1.update(preds, labels)\n",
    "            precision.update(preds, labels)\n",
    "            recall.update(preds, labels)\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        avg_train_accuracy = train_accuracy / len(train_loader.dataset)\n",
    "        mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_acc\", avg_train_accuracy, step=epoch)\n",
    "\n",
    "        f1_res = f1.compute()\n",
    "        precision_res = precision.compute()\n",
    "        recall_res = recall.compute()\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            mlflow.log_metric(\n",
    "                f\"train_f1_{ids2labels[i]}\", f1_res[i], step=epoch\n",
    "            )\n",
    "            mlflow.log_metric(\n",
    "                f\"train_precision_{ids2labels[i]}\",\n",
    "                precision_res[i],\n",
    "                step=epoch,\n",
    "            )\n",
    "            mlflow.log_metric(\n",
    "                f\"train_recall_{ids2labels[i]}\", recall_res[i], step=epoch\n",
    "            )\n",
    "\n",
    "        macro_f1 = f1_res.mean()\n",
    "        macro_precision = precision_res.mean()\n",
    "        macro_recall = recall_res.mean()\n",
    "\n",
    "        mlflow.log_metric(\"train_f1\", macro_f1, step=epoch)\n",
    "        mlflow.log_metric(\"train_precision\", macro_precision, step=epoch)\n",
    "        mlflow.log_metric(\"train_recall\", macro_recall, step=epoch)\n",
    "\n",
    "        tqdm.write(\n",
    "            f\"Epoch {epoch+1}: \"\n",
    "            f\"Training Loss: {avg_loss}, \"\n",
    "            f\"Training Accuracy: {avg_train_accuracy}, \"\n",
    "            f\"Training F1: {macro_f1}, \"\n",
    "            f\"Training Precision: {macro_precision}, \"\n",
    "            f\"Training Recall: {macro_recall}\"\n",
    "        )\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()  # Switch to evaluation mode\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "\n",
    "        f1.reset()\n",
    "        precision.reset()\n",
    "        recall.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(\n",
    "                val_loader,\n",
    "                desc=f\"Validation of epoch {epoch + 1}\",\n",
    "                leave=False,\n",
    "            ):\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                num_metadata = batch[\"num_metadata\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask, num_metadata)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                preds = torch.argmax(outputs, dim=-1)\n",
    "                val_accuracy += (preds == labels).sum().item()\n",
    "                f1.update(preds, labels)\n",
    "                precision.update(preds, labels)\n",
    "                recall.update(preds, labels)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_accuracy = val_accuracy / len(val_loader.dataset)\n",
    "        mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_acc\", avg_val_accuracy, step=epoch)\n",
    "\n",
    "        f1_res = f1.compute()\n",
    "        precision_res = precision.compute()\n",
    "        recall_res = recall.compute()\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            mlflow.log_metric(\n",
    "                f\"val_f1_{ids2labels[i]}\", f1_res[i], step=epoch\n",
    "            )\n",
    "            mlflow.log_metric(\n",
    "                f\"val_precision_{ids2labels[i]}\",\n",
    "                precision_res[i],\n",
    "                step=epoch,\n",
    "            )\n",
    "            mlflow.log_metric(\n",
    "                f\"val_recall_{ids2labels[i]}\", recall_res[i], step=epoch\n",
    "            )\n",
    "\n",
    "        macro_f1 = f1_res.mean()\n",
    "        macro_precision = precision_res.mean()\n",
    "        macro_recall = recall_res.mean()\n",
    "\n",
    "        mlflow.log_metric(\"val_f1\", macro_f1, step=epoch)\n",
    "        mlflow.log_metric(\"val_precision\", macro_precision, step=epoch)\n",
    "        mlflow.log_metric(\"val_recall\", macro_recall, step=epoch)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}: \"\n",
    "            f\"Validation Loss: {avg_val_loss}, \"\n",
    "            f\"Validation Accuracy: {avg_val_accuracy}, \"\n",
    "            f\"Validation F1: {macro_f1}, \"\n",
    "            f\"Validation Precision: {macro_precision}, \"\n",
    "            f\"Validation Recall: {macro_recall}\"\n",
    "        )\n",
    "\n",
    "        save_checkpoint(\n",
    "            model, optimizer, epoch, avg_val_accuracy, checkpoint_path\n",
    "        )\n",
    "        if (epoch + 1) % checkpoint_send_interval == 0:# and epoch != 0:\n",
    "            save_model_remotely(checkpoint_path, remote_models_path, creds)\n",
    "\n",
    "        # Check for early stopping\n",
    "        if avg_val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = avg_val_accuracy\n",
    "            patience_counter = 0\n",
    "            # Save the best model\n",
    "            save_best_model(\n",
    "                model,\n",
    "                optimizer,\n",
    "                epoch,\n",
    "                best_val_accuracy,\n",
    "                best_model_path\n",
    "            )\n",
    "            save_model_remotely(best_model_path, remote_models_path, creds)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Log final checkpoint\n",
    "    save_model_remotely(checkpoint_path, remote_models_path, creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T16:15:04.505587Z",
     "iopub.status.busy": "2025-05-12T16:15:04.504822Z",
     "iopub.status.idle": "2025-05-12T16:25:33.092317Z",
     "shell.execute_reply": "2025-05-12T16:25:33.091469Z",
     "shell.execute_reply.started": "2025-05-12T16:15:04.505556Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Resume is False. Starting from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 1.7874982580542564, Training Accuracy: 0.166, Training F1: 0.11546405404806137, Training Precision: 0.18935900926589966, Training Recall: 0.16144855320453644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss: 1.7787089518138341, Validation Accuracy: 0.20015576323987538, Validation F1: 0.09243802726268768, Validation Precision: 0.06661953777074814, Validation Recall: 0.1681663691997528\n",
      "Checkpoint saved at epoch 0 with validation accuracy 0.2002\n",
      "Best model saved at epoch 0 with validation accuracy 0.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading /kaggle/working/best_model_5.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410k/410k [00:03<00:00, 133kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik best_model_5.pth zostaÅ‚ wysÅ‚any.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training Loss: 1.7724933922290802, Training Accuracy: 0.192, Training F1: 0.11313014477491379, Training Precision: 0.14845968782901764, Training Recall: 0.16824977099895477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Loss: 1.7714020354407174, Validation Accuracy: 0.20482866043613707, Validation F1: 0.057922448962926865, Validation Precision: 0.07578125596046448, Validation Recall: 0.16670499742031097\n",
      "Checkpoint saved at epoch 1 with validation accuracy 0.2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading checkpoint_5.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410k/410k [00:02<00:00, 142kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik checkpoint_5.pth zostaÅ‚ wysÅ‚any.\n",
      "Best model saved at epoch 1 with validation accuracy 0.2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading /kaggle/working/best_model_5.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410k/410k [00:02<00:00, 175kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik best_model_5.pth zostaÅ‚ wysÅ‚any.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Training Loss: 1.762019120156765, Training Accuracy: 0.217, Training F1: 0.09672820568084717, Training Precision: 0.07129691541194916, Training Recall: 0.1763232946395874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Loss: 1.7656518618265789, Validation Accuracy: 0.20171339563862928, Validation F1: 0.07219257950782776, Validation Precision: 0.0736996978521347, Validation Recall: 0.17340753972530365\n",
      "Checkpoint saved at epoch 2 with validation accuracy 0.2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Training Loss: 1.764833115041256, Training Accuracy: 0.221, Training F1: 0.08182398974895477, Training Precision: 0.2435411810874939, Training Recall: 0.176616370677948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Loss: 1.7644382601692563, Validation Accuracy: 0.21261682242990654, Validation F1: 0.09381765127182007, Validation Precision: 0.07657727599143982, Validation Recall: 0.18132130801677704\n",
      "Checkpoint saved at epoch 3 with validation accuracy 0.2126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading checkpoint_5.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410k/410k [00:03<00:00, 122kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik checkpoint_5.pth zostaÅ‚ wysÅ‚any.\n",
      "Best model saved at epoch 3 with validation accuracy 0.2126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading /kaggle/working/best_model_5.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410k/410k [00:02<00:00, 161kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik best_model_5.pth zostaÅ‚ wysÅ‚any.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Training Loss: 1.768201008439064, Training Accuracy: 0.221, Training F1: 0.10286553204059601, Training Precision: 0.1345103532075882, Training Recall: 0.18172161281108856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Validation Loss: 1.7691308770860945, Validation Accuracy: 0.20249221183800623, Validation F1: 0.10128641128540039, Validation Precision: 0.1132204532623291, Validation Recall: 0.17285558581352234\n",
      "Checkpoint saved at epoch 4 with validation accuracy 0.2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Training Loss: 1.7627163752913475, Training Accuracy: 0.22, Training F1: 0.10413183271884918, Training Precision: 0.11961089074611664, Training Recall: 0.1769745647907257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Validation Loss: 1.7615349690119426, Validation Accuracy: 0.19626168224299065, Validation F1: 0.08665351569652557, Validation Precision: 0.112205371260643, Validation Recall: 0.16847579181194305\n",
      "Checkpoint saved at epoch 5 with validation accuracy 0.1963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading checkpoint_5.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410k/410k [00:03<00:00, 136kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik checkpoint_5.pth zostaÅ‚ wysÅ‚any.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Training Loss: 1.7655437588691711, Training Accuracy: 0.23, Training F1: 0.10161914676427841, Training Precision: 0.12798887491226196, Training Recall: 0.17780795693397522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Validation Loss: 1.7648552826472692, Validation Accuracy: 0.20249221183800623, Validation F1: 0.10398461669683456, Validation Precision: 0.11340431869029999, Validation Recall: 0.17275482416152954\n",
      "Checkpoint saved at epoch 6 with validation accuracy 0.2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Training Loss: 1.7597672864794731, Training Accuracy: 0.23, Training F1: 0.11277897655963898, Training Precision: 0.11982901394367218, Training Recall: 0.18078330159187317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Validation Loss: 1.7564453454244704, Validation Accuracy: 0.20404984423676012, Validation F1: 0.10925977677106857, Validation Precision: 0.11320169270038605, Validation Recall: 0.17379848659038544\n",
      "Checkpoint saved at epoch 7 with validation accuracy 0.2040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading checkpoint_5.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410k/410k [00:03<00:00, 122kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik checkpoint_5.pth zostaÅ‚ wysÅ‚any.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Training Loss: 1.7602219879627228, Training Accuracy: 0.233, Training F1: 0.11558737605810165, Training Precision: 0.12283148616552353, Training Recall: 0.18270424008369446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Validation Loss: 1.7652590558642434, Validation Accuracy: 0.20015576323987538, Validation F1: 0.09851672500371933, Validation Precision: 0.11193466186523438, Validation Recall: 0.17103110253810883\n",
      "Checkpoint saved at epoch 8 with validation accuracy 0.2002\n",
      "Early stopping at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading checkpoint_5.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410k/410k [00:02<00:00, 140kB/s]  \n",
      "/kaggle/input/liar-plus-utils/utils.py:90: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plik checkpoint_5.pth zostaÅ‚ wysÅ‚any.\n",
      "Model loaded from best model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:24<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7544, Test Accuracy: 0.2175, Test F1: tensor([0.0000, 0.2340, 0.0000, 0.3368, 0.0000, 0.0000], device='cuda:0') (marcro = 0.0951), Test Precision: tensor([0.0000, 0.2500, 0.0000, 0.2107, 0.0000, 0.0000], device='cuda:0') (marcro = 0.0768), Test Recall: tensor([0.0000, 0.2200, 0.0000, 0.8390, 0.0000, 0.0000], device='cuda:0') (marcro = 0.1765), \n",
      "ðŸƒ View run hilarious-cow-179 at: http://cimmerian.win:5000/#/experiments/3/runs/d88678500519485c94d1c6af0ad57bcc\n",
      "ðŸ§ª View experiment at: http://cimmerian.win:5000/#/experiments/3\n",
      "Total time took training: 627.2867407798767s\n"
     ]
    }
   ],
   "source": [
    "mlflow_uri = \"http://cimmerian.win:5000\"\n",
    "resume = False\n",
    "reset_epoch = False\n",
    "\n",
    "creds = {\n",
    "    'hostname': \"cimmerian.win\",\n",
    "    'port': 22,\n",
    "    'username': \"conan\",\n",
    "    'password': \"conan\"\n",
    "}\n",
    "\n",
    "mlflow.set_tracking_uri(uri=mlflow_uri)\n",
    "\n",
    "# MLflow experiment setup\n",
    "mlflow.set_experiment(\"LiarPlusSingleRoBERTasClassifier\")\n",
    "\n",
    "# Load RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "for param in roberta.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = 6\n",
    "lr = 1e-3\n",
    "epochs = 30\n",
    "hidden_size = 128\n",
    "# Number of epochs to wait before stopping if no improvement\n",
    "patience = 10\n",
    "\n",
    "# Save path\n",
    "save_path = \"/kaggle/working\"\n",
    "# Remote models path\n",
    "remote_models_path = \"/home/conan/models/single_roberta/\"\n",
    "# Best model path\n",
    "best_model_path = f\"{save_path}/best_model_{patience}.pth\"\n",
    "\n",
    "# moÅ¼na przetestowaÄ‡ zachÅ‚annie\n",
    "# dodajemy kolumnÄ™ jak poprawia i nie dodajemy jak nie poprawia\n",
    "text_columns = [\n",
    "    \"statement\",\n",
    "    \"subject\",\n",
    "    \"speaker\",\n",
    "    \"job_title\",\n",
    "    \"state\",\n",
    "    \"party_affiliation\",\n",
    "    \"context\",\n",
    "    \"justification\",\n",
    "]\n",
    "num_metadata_cols = [\n",
    "    \"barely_true_counts\",\n",
    "    \"false_counts\",\n",
    "    \"half_true_counts\",\n",
    "    \"mostly_true_counts\",\n",
    "    \"pants_on_fire_counts\",\n",
    "]\n",
    "\n",
    "subset_size = 1000\n",
    "random_state = 42\n",
    "\n",
    "# speedup the experiments\n",
    "# moÅ¼na ustawiÄ‡ epochs na 1 i sprawdziÄ‡ czy w ramach jednej epoki val loss spada\n",
    "training_data = LiarPlusSingleRobertaDataset(\n",
    "    \"/kaggle/input/liar-plus-normalized-2/train2.csv\",\n",
    "    tokenizer,\n",
    "    text_columns,\n",
    "    num_metadata_cols\n",
    ")\n",
    "validation_data = LiarPlusSingleRobertaDataset(\n",
    "    \"/kaggle/input/liar-plus-normalized-2/val2.csv\",\n",
    "    tokenizer,\n",
    "    text_columns,\n",
    "    num_metadata_cols,\n",
    ")\n",
    "test_data = LiarPlusSingleRobertaDataset(\n",
    "    \"/kaggle/input/liar-plus-normalized-2/test2.csv\",\n",
    "    tokenizer,\n",
    "    text_columns,\n",
    "    num_metadata_cols,\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "#training_data_subset = Subset(training_data, sample(range(len(training_data)), k=1000))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    training_data, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    validation_data, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# Instantiate model\n",
    "model = LiarPlusSingleRoBERTasClassifier(\n",
    "    roberta,\n",
    "    len(num_metadata_cols),\n",
    "    hidden_size,\n",
    "    num_classes,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "start = time.time()\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"learning_rate\", lr)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"resume\", resume)\n",
    "    mlflow.log_param(\"reset_epoch\", reset_epoch)\n",
    "    mlflow.log_param(\"patience\", patience)\n",
    "    \n",
    "    # Train the model\n",
    "    train(\n",
    "        creds,\n",
    "        model,\n",
    "        save_path,\n",
    "        remote_models_path,\n",
    "        best_model_path,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        test_dataloader,\n",
    "        batch_size,\n",
    "        num_classes,\n",
    "        lr,\n",
    "        epochs,\n",
    "        patience,\n",
    "        resume,\n",
    "        reset_epoch,\n",
    "    )\n",
    "    # Evaluate on test dataset\n",
    "    test(model, best_model_path, test_dataloader)\n",
    "end = time.time()\n",
    "print(f\"Total time took training: {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7396558,
     "sourceId": 11781193,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7399498,
     "sourceId": 11785259,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
