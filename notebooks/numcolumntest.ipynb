{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12015308,"sourceType":"datasetVersion","datasetId":7559194}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mlflow\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, Subset\nfrom random import sample\nfrom torch.utils.data import DataLoader\nfrom transformers import RobertaModel, RobertaTokenizer\nfrom sklearn.utils import resample\nfrom torchmetrics.classification import (\n    MulticlassF1Score,\n    MulticlassPrecision,\n    MulticlassRecall,\n)\nfrom tqdm import tqdm\nimport mlflow\nimport time\nimport pandas as pd\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:17:13.888241Z","iopub.execute_input":"2025-05-31T19:17:13.888518Z","iopub.status.idle":"2025-05-31T19:17:52.597345Z","shell.execute_reply.started":"2025-05-31T19:17:13.888497Z","shell.execute_reply":"2025-05-31T19:17:52.596763Z"}},"outputs":[{"name":"stdout","text":"Collecting mlflow\n  Downloading mlflow-2.22.0-py3-none-any.whl.metadata (30 kB)\nCollecting mlflow-skinny==2.22.0 (from mlflow)\n  Downloading mlflow_skinny-2.22.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\nRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.6)\nRequirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\nRequirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\nCollecting graphene<4 (from mlflow)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\nRequirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7.2)\nRequirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\nRequirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.3)\nRequirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.2.2)\nRequirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.40)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (5.5.2)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (8.1.8)\nRequirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.1.1)\nCollecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.22.0->mlflow)\n  Downloading databricks_sdk-0.55.0-py3-none-any.whl.metadata (39 kB)\nCollecting fastapi<1 (from mlflow-skinny==2.22.0->mlflow)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.1.44)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (8.7.0)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (1.31.1)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (1.31.1)\nCollecting packaging<25 (from mlflow-skinny==2.22.0->mlflow)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.20.3)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (2.11.4)\nRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (6.0.2)\nRequirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (2.32.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (0.5.3)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (4.13.2)\nCollecting uvicorn<1 (from mlflow-skinny==2.22.0->mlflow)\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\nRequirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.0.9)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\nRequirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (2.40.1)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.22.0->mlflow)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow) (4.0.12)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.0->mlflow) (3.21.0)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (1.2.18)\nCollecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.22.0->mlflow)\n  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (0.52b1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (0.4.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (2025.4.26)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==2.22.0->mlflow) (0.14.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3->mlflow) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (1.17.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow) (5.0.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (4.9.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow) (4.9.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow) (1.3.1)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.6.1)\nDownloading mlflow-2.22.0-py3-none-any.whl (29.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading mlflow_skinny-2.22.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading databricks_sdk-0.55.0-py3-none-any.whl (722 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m722.9/722.9 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\nDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: uvicorn, packaging, importlib_metadata, graphql-core, starlette, gunicorn, graphql-relay, graphene, fastapi, databricks-sdk, mlflow-skinny, mlflow\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: importlib_metadata\n    Found existing installation: importlib_metadata 8.7.0\n    Uninstalling importlib_metadata-8.7.0:\n      Successfully uninstalled importlib_metadata-8.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed databricks-sdk-0.55.0 fastapi-0.115.12 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 importlib_metadata-8.6.1 mlflow-2.22.0 mlflow-skinny-2.22.0 packaging-24.2 starlette-0.46.2 uvicorn-0.34.2\n","output_type":"stream"},{"name":"stderr","text":"2025-05-31 19:17:37.923257: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748719058.107504      75 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748719058.159509      75 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"LABEL_MAPPING = {\n    \"pants-fire\": 0,\n    \"false\": 1,\n    \"barely-true\": 2,\n    \"half-true\": 3,\n    \"mostly-true\": 4,\n    \"true\": 5,\n}\n\nids2labels = [\n    \"pants-fire\",\n    \"false\",\n    \"barely-true\",\n    \"half-true\",\n    \"mostly-true\",\n    \"true\",\n]\n\n\ndef save_checkpoint(model, optimizer, epoch, val_acc, path=\"checkpoint.pth\"):\n    checkpoint = {\n        \"model_state_dict\": model.state_for_save(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"epoch\": epoch,\n        \"val_acc\": val_acc,\n    }\n    torch.save(checkpoint, path)\n    print(\n        f\"Checkpoint saved at epoch {epoch} \"\n        f\"with validation accuracy {val_acc:.4f}\"\n    )\n\n\ndef load_checkpoint(\n    model, optimizer, path=\"checkpoint.pth\", resume=False, reset_epoch=False\n):\n    if not resume:\n        print(\"Resume is False. Starting from scratch.\")\n        return 0, 0  # Start fresh\n\n    if os.path.exists(path):\n        checkpoint = torch.load(path)\n        model.load_state_from_save(checkpoint[\"model_state_dict\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        epoch = checkpoint[\"epoch\"]\n        val_acc = checkpoint[\"val_acc\"]\n        if reset_epoch:\n            print(\n                f\"Checkpoint loaded: Starting from initial\"\n                f\"epoch, validation accuracy {val_acc:.4f}\"\n            )\n            return 0, val_acc  # Start fresh with existing model\n        else:\n            print(\n                f\"Checkpoint loaded: Resuming from epoch \"\n                f\"{epoch+1}, validation accuracy {val_acc:.4f}\"\n            )\n            return epoch + 1, val_acc  # Next epoch to train\n    else:\n        print(\"No checkpoint found. Starting from scratch.\")\n        return 0, 0  # Start fresh\n\n\ndef save_best_model(model, optimizer, epoch, val_acc, path=\"best_model.pth\"):\n    best_model = {\n        \"model_state_dict\": model.state_for_save(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"epoch\": epoch,\n        \"val_acc\": val_acc,\n    }\n    torch.save(best_model, path)\n    print(\n        f\"Best model saved at epoch {epoch} \"\n        f\"with validation accuracy {val_acc:.4f}\"\n    )\n\n\ndef load_best_model(model, path=\"best_model.pth\"):\n    if os.path.exists(path):\n        best_model = torch.load(path)\n        model.load_state_from_save(best_model[\"model_state_dict\"])\n        print(\"Model loaded from best model checkpoint.\")\n    else:\n        print(\"No best model checkpoint found.\")\n\ndef save_model_remotely(local_path, remote_path, creds):\n    pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:18:29.657139Z","iopub.execute_input":"2025-05-31T19:18:29.657771Z","iopub.status.idle":"2025-05-31T19:18:29.666721Z","shell.execute_reply.started":"2025-05-31T19:18:29.657742Z","shell.execute_reply":"2025-05-31T19:18:29.665932Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class PrepareDataset(Dataset):\n    def __init__(\n        self,\n        filepath: str,\n        num_metadata_col: str,\n    ):\n        self.df = pd.read_csv(filepath)\n        self.num_metadata_col = num_metadata_col\n\n    def __len__(self):\n        return len(self.df.index)\n\n    def __getitem__(self, index: int):\n        item = self.df.iloc[index]\n        column = self.num_metadata_col\n\n        return {\n            \"num_metadata\": torch.tensor([item[column]], dtype=torch.float32),\n            \"label\": torch.tensor(LABEL_MAPPING[item[\"label\"]]),\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:18:33.496712Z","iopub.execute_input":"2025-05-31T19:18:33.497300Z","iopub.status.idle":"2025-05-31T19:18:33.502256Z","shell.execute_reply.started":"2025-05-31T19:18:33.497271Z","shell.execute_reply":"2025-05-31T19:18:33.501338Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Classifier(nn.Module):\n    def __init__(self, num_metadata_len, num_hidden, num_classes):\n        super(Classifier, self).__init__()\n        self.hl = nn.Linear(num_metadata_len, num_hidden)\n        self.fc = nn.Linear(num_hidden, num_classes)\n\n    def forward(self, num_metadata):\n        hl_output = F.gelu(self.hl(num_metadata))\n        logits = self.fc(hl_output)\n        return logits\n\n    def state_for_save(self):\n        return {\n            'hl_state_dict': self.hl.state_dict(),\n            'fc_state_dict': self.fc.state_dict(),\n        }\n        \n    def load_state_from_save(self, state):\n        self.hl.load_state_dict(state['hl_state_dict'])\n        self.fc.load_state_dict(state['fc_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:18:35.911520Z","iopub.execute_input":"2025-05-31T19:18:35.912277Z","iopub.status.idle":"2025-05-31T19:18:35.917705Z","shell.execute_reply.started":"2025-05-31T19:18:35.912250Z","shell.execute_reply":"2025-05-31T19:18:35.916922Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def train(\n    creds: dict,\n    model: nn.Module,\n    save_path: str,\n    remote_models_path: str,\n    best_model_path: str,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    test_loader: DataLoader,\n    batch_size: int,\n    num_classes: int,\n    lr=1e-3,\n    epochs=30,\n    patience=5,\n    resume: bool = False,\n    reset_epoch: bool = False,\n) -> None:\n    dev_name = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device {dev_name}\")\n    device = torch.device(dev_name)\n\n    # Define optimizer and loss function\n    # Train only the classifier\n    optimizer = torch.optim.Adam(model.fc.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    # Checkpoint Path\n    checkpoint_path = f\"checkpoint_{patience}.pth\"\n\n    checkpoint_send_interval = 2\n\n    # Track best loss for model saving\n    # Load Checkpoint (Decide if you want to continue)\n    start_epoch, best_val_accuracy = load_checkpoint(\n        model,\n        optimizer,\n        checkpoint_path,\n        resume,\n        reset_epoch\n    )\n\n    patience_counter = 0\n\n    f1 = MulticlassF1Score(num_classes, average=None).to(device)\n    precision = MulticlassPrecision(num_classes, average=None).to(device)\n    recall = MulticlassRecall(num_classes, average=None).to(device)\n\n    # Training loop\n    for epoch in range(start_epoch, epochs):\n        model.train()\n        epoch_loss = 0\n\n        train_accuracy = 0\n\n        for batch in tqdm(\n            train_loader, desc=f\"Epoch {epoch+1}\", leave=False\n        ):\n            num_metadata = batch[\"num_metadata\"].to(device)\n            labels = batch[\"label\"].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(num_metadata)\n            loss = criterion(\n                outputs, labels\n            )  # moÅ¼na sprÃ³bowaÄ‡ to logowaÄ‡ jako osobny wykres do debugowania\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n\n            # Calculate accuracy\n            preds = torch.argmax(outputs, dim=-1)\n            train_accuracy += (preds == labels).sum().item()\n\n            f1.update(preds, labels)\n            precision.update(preds, labels)\n            recall.update(preds, labels)\n\n        avg_loss = epoch_loss / len(train_loader)\n        avg_train_accuracy = train_accuracy / len(train_loader.dataset)\n        mlflow.log_metric(\"train_loss\", avg_loss, step=epoch)\n        mlflow.log_metric(\"train_acc\", avg_train_accuracy, step=epoch)\n\n        f1_res = f1.compute()\n        precision_res = precision.compute()\n        recall_res = recall.compute()\n\n        for i in range(num_classes):\n            mlflow.log_metric(\n                f\"train_f1_{ids2labels[i]}\", f1_res[i], step=epoch\n            )\n            mlflow.log_metric(\n                f\"train_precision_{ids2labels[i]}\",\n                precision_res[i],\n                step=epoch,\n            )\n            mlflow.log_metric(\n                f\"train_recall_{ids2labels[i]}\", recall_res[i], step=epoch\n            )\n\n        macro_f1 = f1_res.mean()\n        macro_precision = precision_res.mean()\n        macro_recall = recall_res.mean()\n\n        mlflow.log_metric(\"train_f1\", macro_f1, step=epoch)\n        mlflow.log_metric(\"train_precision\", macro_precision, step=epoch)\n        mlflow.log_metric(\"train_recall\", macro_recall, step=epoch)\n\n        tqdm.write(\n            f\"Epoch {epoch+1}: \"\n            f\"Training Loss: {avg_loss}, \"\n            f\"Training Accuracy: {avg_train_accuracy}, \"\n            f\"Training F1: {macro_f1}, \"\n            f\"Training Precision: {macro_precision}, \"\n            f\"Training Recall: {macro_recall}\"\n        )\n\n        # Validation step\n        model.eval()  # Switch to evaluation mode\n        val_loss = 0\n        val_accuracy = 0\n\n        f1.reset()\n        precision.reset()\n        recall.reset()\n\n        with torch.no_grad():\n            for batch in tqdm(\n                val_loader,\n                desc=f\"Validation of epoch {epoch + 1}\",\n                leave=False,\n            ):\n                num_metadata = batch[\"num_metadata\"].to(device)\n                labels = batch[\"label\"].to(device)\n\n                outputs = model(num_metadata)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                # Calculate accuracy\n                preds = torch.argmax(outputs, dim=-1)\n                val_accuracy += (preds == labels).sum().item()\n                f1.update(preds, labels)\n                precision.update(preds, labels)\n                recall.update(preds, labels)\n\n        avg_val_loss = val_loss / len(val_loader)\n        avg_val_accuracy = val_accuracy / len(val_loader.dataset)\n        mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch)\n        mlflow.log_metric(\"val_acc\", avg_val_accuracy, step=epoch)\n\n        f1_res = f1.compute()\n        precision_res = precision.compute()\n        recall_res = recall.compute()\n\n        for i in range(num_classes):\n            mlflow.log_metric(\n                f\"val_f1_{ids2labels[i]}\", f1_res[i], step=epoch\n            )\n            mlflow.log_metric(\n                f\"val_precision_{ids2labels[i]}\",\n                precision_res[i],\n                step=epoch,\n            )\n            mlflow.log_metric(\n                f\"val_recall_{ids2labels[i]}\", recall_res[i], step=epoch\n            )\n\n        macro_f1 = f1_res.mean()\n        macro_precision = precision_res.mean()\n        macro_recall = recall_res.mean()\n\n        mlflow.log_metric(\"val_f1\", macro_f1, step=epoch)\n        mlflow.log_metric(\"val_precision\", macro_precision, step=epoch)\n        mlflow.log_metric(\"val_recall\", macro_recall, step=epoch)\n\n        print(\n            f\"Epoch {epoch+1}: \"\n            f\"Validation Loss: {avg_val_loss}, \"\n            f\"Validation Accuracy: {avg_val_accuracy}, \"\n            f\"Validation F1: {macro_f1}, \"\n            f\"Validation Precision: {macro_precision}, \"\n            f\"Validation Recall: {macro_recall}\"\n        )\n\n        save_checkpoint(\n            model, optimizer, epoch, avg_val_accuracy, checkpoint_path\n        )\n        if (epoch + 1) % checkpoint_send_interval == 0:# and epoch != 0:\n            save_model_remotely(checkpoint_path, remote_models_path, creds)\n\n        # Check for early stopping\n        if avg_val_accuracy > best_val_accuracy:\n            best_val_accuracy = avg_val_accuracy\n            patience_counter = 0\n            # Save the best model\n            save_best_model(\n                model,\n                optimizer,\n                epoch,\n                best_val_accuracy,\n                best_model_path\n            )\n            save_model_remotely(best_model_path, remote_models_path, creds)\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    # Log final checkpoint\n    save_model_remotely(checkpoint_path, remote_models_path, creds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:18:38.338722Z","iopub.execute_input":"2025-05-31T19:18:38.339269Z","iopub.status.idle":"2025-05-31T19:18:38.355764Z","shell.execute_reply.started":"2025-05-31T19:18:38.339244Z","shell.execute_reply":"2025-05-31T19:18:38.354965Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def test(\n    model: nn.Module,\n    best_model_path: str,\n    dataloader: DataLoader\n) -> None:\n    # Define loss function\n    criterion = nn.CrossEntropyLoss()\n\n    load_best_model(model, best_model_path)\n    \n    model.eval()  # Set model to evaluation mode\n    total_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n\n    f1 = MulticlassF1Score(num_classes, average=None).to(device)\n    precision = MulticlassPrecision(num_classes, average=None).to(device)\n    recall = MulticlassRecall(num_classes, average=None).to(device)\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            num_metadata = batch[\"num_metadata\"].to(device)\n            labels = batch[\"label\"].to(device)\n\n            outputs = model(num_metadata)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * num_metadata.size(0)\n\n            preds = torch.argmax(outputs, dim=1)\n            total_correct += (preds == labels).sum().item()\n            total_samples += num_metadata.size(0)\n\n            f1.update(preds, labels)\n            precision.update(preds, labels)\n            recall.update(preds, labels)\n\n    avg_loss = total_loss / total_samples\n    accuracy = total_correct / total_samples\n\n    f1_res = f1.compute()\n    precision_res = precision.compute()\n    recall_res = recall.compute()\n\n    mlflow.log_metric(\"test_acc\", accuracy)\n    mlflow.log_metric(\"test_loss\", avg_loss)\n\n    for i in range(num_classes):\n        mlflow.log_metric(f\"test_f1_{ids2labels[i]}\", f1_res[i])\n        mlflow.log_metric(f\"test_precision_{ids2labels[i]}\", precision_res[i])\n        mlflow.log_metric(f\"test_recall_{ids2labels[i]}\", recall_res[i])\n    \n    macro_f1 = f1_res.mean()\n    macro_precision = precision_res.mean()\n    macro_recall = recall_res.mean()\n\n    mlflow.log_metric(\"test_f1\", macro_f1)\n    mlflow.log_metric(\"test_precision\", macro_precision)\n    mlflow.log_metric(\"test_recall\", macro_recall)\n\n    print(\n        f\"Test Loss: {avg_loss:.4f}, \"\n        f\"Test Accuracy: {accuracy:.4f}, \"\n        f\"Test F1: {f1_res} (marcro = {macro_f1:.4f}), \"\n        f\"Test Precision: {precision_res} (marcro = {macro_precision:.4f}), \"\n        f\"Test Recall: {recall_res} (marcro = {macro_recall:.4f}), \"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:29:19.120200Z","iopub.execute_input":"2025-05-31T19:29:19.120983Z","iopub.status.idle":"2025-05-31T19:29:19.129494Z","shell.execute_reply.started":"2025-05-31T19:29:19.120956Z","shell.execute_reply":"2025-05-31T19:29:19.128831Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def do_experiment(training_data, validation_data, test_data):\n    # MLflow experiment setup\n    print(f\"Starting ColumnTest_{num_column}\")\n\n    mlflow_uri = \"http://cimmerian.win:5000\"\n    resume = False\n    reset_epoch = False\n\n    creds = {\n        'hostname': \"cimmerian.win\",\n        'port': 22,\n        'username': \"conan\",\n        'password': \"conan\"\n    }\n    \n    mlflow.set_tracking_uri(uri=mlflow_uri)\n    mlflow.set_experiment(f\"ColumnTest_{num_column}\")\n        \n    \n        \n    train_dataloader = DataLoader(\n        training_data, batch_size=batch_size, shuffle=True\n    )\n    val_dataloader = DataLoader(\n        validation_data, batch_size=batch_size, shuffle=True\n    )\n    test_dataloader = DataLoader(\n        test_data, batch_size=batch_size, shuffle=True\n    )\n    \n    # Instantiate model\n    model = Classifier(\n    num_metadata_len=1,  # jedna kolumna numeryczna\n    num_hidden=hidden_size,\n    num_classes=num_classes,\n    )\n\n    model.to(device)\n    \n    start = time.time()\n    with mlflow.start_run():\n        mlflow.log_param(\"learning_rate\", lr)\n        mlflow.log_param(\"batch_size\", batch_size)\n        mlflow.log_param(\"epochs\", epochs)\n        mlflow.log_param(\"resume\", resume)\n        mlflow.log_param(\"reset_epoch\", reset_epoch)\n        mlflow.log_param(\"patience\", patience)\n        \n        # Train the model\n        train(\n            creds,\n            model,\n            save_path,\n            remote_models_path,\n            best_model_path,\n            train_dataloader,\n            val_dataloader,\n            test_dataloader,\n            batch_size,\n            num_classes,\n            lr,\n            epochs,\n            patience,\n            resume,\n            reset_epoch,\n        )\n        # Evaluate on test dataset\n        test(model, best_model_path, test_dataloader)\n    end = time.time()\n    print(f\"Total time took training: {end-start}s\")\n\n# Hyperparameters\nnum_classes = 6\nlr = 1e-3\nepochs = 64\nhidden_size = 128\n# Number of epochs to wait before stopping if no improvement\npatience = 10\n\n# Save path\nsave_path = \"/kaggle/working\"\n# Remote models path\nremote_models_path = \"/home/conan/models/single_roberta/\"\n# Best model path\nbest_model_path = f\"{save_path}/best_model_{patience}.pth\"\n\nbatch_size = 64\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnum_metadata_cols = [\n    # \"barely_true_counts\",\n    # \"false_counts\",\n    # \"half_true_counts\",\n    # \"mostly_true_counts\",\n    # \"pants_on_fire_counts\",\n    \"grammar_errors\",\n    # \"ratio_of_capital_letters\"\n]    \nfor num_column in num_metadata_cols:    \n    training_data = PrepareDataset(\n        \"/kaggle/input/normalized/train2.csv\",\n        num_metadata_col=num_column\n    )\n    validation_data = PrepareDataset(\n        \"/kaggle/input/normalized/val2.csv\",\n        num_metadata_col=num_column\n    )\n    test_data = PrepareDataset(\n        \"/kaggle/input/normalized/test2.csv\",\n        num_metadata_col=num_column\n    )\n    do_experiment(training_data, validation_data, test_data)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T19:51:50.001767Z","iopub.execute_input":"2025-05-31T19:51:50.002608Z","iopub.status.idle":"2025-05-31T19:55:35.438227Z","shell.execute_reply.started":"2025-05-31T19:51:50.002580Z","shell.execute_reply":"2025-05-31T19:55:35.437469Z"}},"outputs":[{"name":"stdout","text":"Starting ColumnTest_grammar_errors\nUsing device cuda\nResume is False. Starting from scratch.\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Training Loss: 1.764677557886017, Training Accuracy: 0.1983640081799591, Training F1: 0.12420692294836044, Training Precision: 0.13900673389434814, Training Recall: 0.16584360599517822\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Validation Loss: 1.7626305194128127, Validation Accuracy: 0.19003115264797507, Validation F1: 0.06935225427150726, Validation Precision: 0.05740497633814812, Validation Recall: 0.16774830222129822\nCheckpoint saved at epoch 0 with validation accuracy 0.1900\nBest model saved at epoch 0 with validation accuracy 0.1900\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Training Loss: 1.7587326987189535, Training Accuracy: 0.2003116174895316, Training F1: 0.11681190133094788, Training Precision: 0.15734918415546417, Training Recall: 0.16555066406726837\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Validation Loss: 1.7740600449698312, Validation Accuracy: 0.20482866043613707, Validation F1: 0.07379080355167389, Validation Precision: 0.06988987326622009, Validation Recall: 0.16784852743148804\nCheckpoint saved at epoch 1 with validation accuracy 0.2048\nBest model saved at epoch 1 with validation accuracy 0.2048\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Training Loss: 1.7590179761744433, Training Accuracy: 0.20021423702405297, Training F1: 0.12225648015737534, Training Precision: 0.1332787722349167, Training Recall: 0.1679529845714569\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Validation Loss: 1.7754141092300415, Validation Accuracy: 0.15186915887850466, Validation F1: 0.07813290506601334, Validation Precision: 0.06834965199232101, Validation Recall: 0.16555756330490112\nCheckpoint saved at epoch 2 with validation accuracy 0.1519\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Training Loss: 1.759071975761319, Training Accuracy: 0.1929107021131561, Training F1: 0.13858753442764282, Training Precision: 0.14932245016098022, Training Recall: 0.16334019601345062\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Validation Loss: 1.782788668360029, Validation Accuracy: 0.20482866043613707, Validation F1: 0.07379080355167389, Validation Precision: 0.06988987326622009, Validation Recall: 0.16784852743148804\nCheckpoint saved at epoch 3 with validation accuracy 0.2048\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Training Loss: 1.7571905932811476, Training Accuracy: 0.20323303145389035, Training F1: 0.13001669943332672, Training Precision: 0.1333484947681427, Training Recall: 0.17060983180999756\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Validation Loss: 1.7690458184196836, Validation Accuracy: 0.20404984423676012, Validation F1: 0.09460711479187012, Validation Precision: 0.06840337812900543, Validation Recall: 0.17459730803966522\nCheckpoint saved at epoch 4 with validation accuracy 0.2040\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Training Loss: 1.759237892139032, Training Accuracy: 0.2003116174895316, Training F1: 0.11412447690963745, Training Precision: 0.13329002261161804, Training Recall: 0.16707448661327362\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Validation Loss: 1.7589995236623854, Validation Accuracy: 0.20404984423676012, Validation F1: 0.09460711479187012, Validation Precision: 0.06840337812900543, Validation Recall: 0.17459730803966522\nCheckpoint saved at epoch 5 with validation accuracy 0.2040\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Training Loss: 1.7580799915775749, Training Accuracy: 0.2014801830752751, Training F1: 0.12349992990493774, Training Precision: 0.13109242916107178, Training Recall: 0.1686570644378662\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Validation Loss: 1.764154388791039, Validation Accuracy: 0.20249221183800623, Validation F1: 0.057574450969696045, Validation Precision: 0.04589426517486572, Validation Recall: 0.16479581594467163\nCheckpoint saved at epoch 6 with validation accuracy 0.2025\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Training Loss: 1.7576564831763322, Training Accuracy: 0.20128542214431785, Training F1: 0.12163765728473663, Training Precision: 0.1290256828069687, Training Recall: 0.16832725703716278\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Validation Loss: 1.7774707476298015, Validation Accuracy: 0.1939252336448598, Validation F1: 0.07115571200847626, Validation Precision: 0.06795349717140198, Validation Recall: 0.167868971824646\nCheckpoint saved at epoch 7 with validation accuracy 0.1939\n","output_type":"stream"},{"name":"stderr","text":"                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Training Loss: 1.758708399275075, Training Accuracy: 0.20216184633362547, Training F1: 0.09958704560995102, Training Precision: 0.12870505452156067, Training Recall: 0.16686289012432098\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Validation Loss: 1.7687133039746965, Validation Accuracy: 0.18925233644859812, Validation F1: 0.06433433294296265, Validation Precision: 0.055295269936323166, Validation Recall: 0.16321811079978943\nCheckpoint saved at epoch 8 with validation accuracy 0.1893\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Training Loss: 1.7581112747607024, Training Accuracy: 0.19924043236926672, Training F1: 0.10461270064115524, Training Precision: 0.1251993179321289, Training Recall: 0.16364917159080505\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Validation Loss: 1.7686002311252413, Validation Accuracy: 0.1954828660436137, Validation F1: 0.09327872097492218, Validation Precision: 0.14999279379844666, Validation Recall: 0.16571715474128723\nCheckpoint saved at epoch 9 with validation accuracy 0.1955\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Training Loss: 1.7589965362726532, Training Accuracy: 0.2010906612133606, Training F1: 0.12173932790756226, Training Precision: 0.13189475238323212, Training Recall: 0.16708795726299286\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Validation Loss: 1.7665294238499232, Validation Accuracy: 0.1939252336448598, Validation F1: 0.0698423832654953, Validation Precision: 0.06976013630628586, Validation Recall: 0.16668711602687836\nCheckpoint saved at epoch 10 with validation accuracy 0.1939\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Training Loss: 1.758607702225632, Training Accuracy: 0.1999220956276171, Training F1: 0.12058179080486298, Training Precision: 0.12799586355686188, Training Recall: 0.16606056690216064\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Validation Loss: 1.7602514369147164, Validation Accuracy: 0.20482866043613707, Validation F1: 0.07379080355167389, Validation Precision: 0.06988987326622009, Validation Recall: 0.16784852743148804\nCheckpoint saved at epoch 11 with validation accuracy 0.2048\nEarly stopping at epoch 12\nModel loaded from best model checkpoint.\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 158.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 1.7517, Test Accuracy: 0.1902, Test F1: tensor([0.0000, 0.3208, 0.0852, 0.0000, 0.0000, 0.0000], device='cuda:0') (marcro = 0.0677), Test Precision: tensor([0.0000, 0.1941, 0.1429, 0.0000, 0.0000, 0.0000], device='cuda:0') (marcro = 0.0562), Test Recall: tensor([0.0000, 0.9240, 0.0607, 0.0000, 0.0000, 0.0000], device='cuda:0') (marcro = 0.1641), \nğŸƒ View run treasured-snake-600 at: http://cimmerian.win:5000/#/experiments/24/runs/3583552bae9745788f8c305113db3914\nğŸ§ª View experiment at: http://cimmerian.win:5000/#/experiments/24\nTotal time took training: 224.86124205589294s\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}